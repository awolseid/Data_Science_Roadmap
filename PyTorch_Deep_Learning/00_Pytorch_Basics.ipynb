{"cells":[{"cell_type":"markdown","metadata":{"id":"jSNK7duj5SeU"},"source":["# **PyTorch Fundamentals**\n","\n","[PyTorch](https://pytorch.org/) is an open source machine learning and deep learning framework.\n","\n","## **What for?**\n","\n","PyTorch allows us to **manipulate and process data** and **write machine learning algorithms** using Python code.\n","\n","## **Who uses?**\n","\n","Many of the worlds largest technology companies such as [Meta](https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/) (Facebook), Tesla and Microsoft as well as artificial intelligence research companies such as [OpenAI](https://openai.com/blog/openai-pytorch/)  use PyTorch to power research and bring machine learning to their products. PyTorch is also used in other industries such as agriculture to power computer vision on [tractors](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1).\n","\n","![pytorch being used across industry and research](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-being-used-across-research-and-industry.png).\n","\n","## **Why?**\n","\n","As of Jun 2023, PyTorch is the most used deep learning framework on [Papers With Code](https://paperswithcode.com/trends), a website for tracking machine learning research papers and the code repositories attached with them.\n","\n","PyTorch also helps take care of many things such as GPU acceleration (making code run faster) behind the scenes.\n","\n"]},{"cell_type":"markdown","source":["## **What to cover in this module**\n","\n","\n","| **Topic** | **Contents** |\n","| ----- | ----- |\n","| **Introduction to tensors** | Tensors are the basic building block of all of machine learning and deep learning. |\n","| **Creating tensors** | Tensors can represent almost any kind of data (images, words, tables of numbers). |\n","| **Information from tensors** | If you can put information into a tensor, you'll want to get it out too. |\n","| **Manipulating tensors** | Machine learning algorithms (like neural networks) involve manipulating tensors in many different ways such as adding, multiplying, combining. |\n","| **Dealing with tensor shapes** | One of the most common issues in machine learning is dealing with shape mismatches (trying to mixed wrong shaped tensors with other tensors). |\n","| **Indexing on tensors** | If you've indexed on a Python list or NumPy array, it's very similar with tensors, except they can have far more dimensions. |\n","| **Mixing tensors and arrays** | PyTorch plays with tensors ([`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html)), NumPy likes arrays ([`np.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)) sometimes you'll want to mix and match these. |\n","| **Reproducibility** | Machine learning is very experimental and since it uses a lot of *randomness* to work, sometimes you'll want that *randomness* to not be so random. |\n","| **Running tensors on GPU** | GPUs (Graphics Processing Units) make your code faster, PyTorch makes it easy to run your code on GPUs. |\n"],"metadata":{"id":"X44fj8QUdETu"}},{"cell_type":"markdown","metadata":{"id":"5v3iRCRUTGeu"},"source":["## **Importing PyTorch**\n","\n","> **Note:** Before running any of the code in this notebook, you should have gone through the [PyTorch setup](https://pytorch.org/get-started/locally/)  steps. However, **if you're running on Google Colab**, everything should work (Google Colab comes with PyTorch and other libraries installed).\n","\n","Let's start by importing PyTorch and checking the version we're using."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1VxEOik46Y4i","outputId":"a8f94d01-1102-4cc5-e829-707b131d545f","executionInfo":{"status":"ok","timestamp":1693718418253,"user_tz":-540,"elapsed":4712,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.1+cu118'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import torch\n","torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"_SqvI4S9TGew"},"source":["Wonderful, it looks like we've got PyTorch 2.0.0+."]},{"cell_type":"markdown","metadata":{"id":"i-33BKR16iWc"},"source":["# **1. Introduction to tensors**\n","\n","Tensors are the fundamental building block of machine learning.\n","Their job is to **represent data in a numerical way**.\n","\n","\n","![example of going from an input image to a tensor representation of the image, image gets broken down into 3 colour channels as well as numbers to represent the height and width](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-tensor-shape-example-of-image.png)\n","\n","For example, we could represent an image as a tensor with shape `[3, 224, 224]` which would mean `[colour_channels, height, width]`, as in the image has `3` colour channels (red, green, blue), a height of `224` pixels and a width of `224` pixels. In tensor-speak (the language used to describe tensors), the tensor would have three dimensions, one for `colour_channels`, `height` and `width`."]},{"cell_type":"markdown","metadata":{"id":"gFF0N2TU7S7Q"},"source":["## **1.1. Creating tensors**\n","\n","PyTorch loves tensors. Read through the documentation on [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html)."]},{"cell_type":"markdown","source":["#### **Scalar**\n","A **scalar** is a single number and in tensor-speak it's a zero dimension tensor."],"metadata":{"id":"cSVrbSLqdNLk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUDgG2zk7Us5","outputId":"a97753e7-633f-4b0e-ace3-f68500d9e30a","executionInfo":{"status":"ok","timestamp":1693719553732,"user_tz":-540,"elapsed":287,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7)"]},"metadata":{},"execution_count":2}],"source":["# Scalar\n","scalar = torch.tensor(7)\n","scalar"]},{"cell_type":"markdown","metadata":{"id":"JqSuhW7rTGey"},"source":["See how the above printed out `tensor(7)`? That means although `scalar` is a single number, it's of type `torch.Tensor`. We can check the dimensions of a tensor using the `ndim` attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lV98Yz868bav","outputId":"fe1c819e-ae43-46fd-e2f9-822bb3b6549e","executionInfo":{"status":"ok","timestamp":1693719556312,"user_tz":-540,"elapsed":310,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":3}],"source":["scalar.ndim # a scalar has no dimension\n"]},{"cell_type":"markdown","metadata":{"id":"ZO2YW_QGTGez"},"source":["What if we wanted to retrieve the number from the tensor? As in, turn it from `torch.Tensor` to a Python integer? To do we can use the `item()` method."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-k4cyKumPfbE","outputId":"1f6a7916-0c7c-403f-8ebd-875454a94470"},"outputs":[{"data":{"text/plain":["7"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Get tensor back as Python number (only works with one-element tensors)\n","scalar.item()"]},{"cell_type":"markdown","metadata":{"id":"qYs7ulrATGe0"},"source":["#### **Vector**.\n","\n","A vector is a **single dimension** tensor but can contain **many numbers**. The important trend here is that a vector is flexible in what it can represent (the same with tensors)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IZF6ASs8QH9","outputId":"e556ed2a-e58a-440f-b103-0f06c91bc75c"},"outputs":[{"data":{"text/plain":["tensor([7, 7])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Vector\n","vector = torch.tensor([7, 7])\n","vector"]},{"cell_type":"markdown","metadata":{"id":"mXxRUUW2TGe1"},"source":["Wonderful, `vector` now contains two 7's. How many dimensions do you think it'll have?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03hm3VVv8kr4","outputId":"2035bb26-0189-4b28-fa02-34220d44677f"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Check the number of dimensions of vector\n","vector.ndim"]},{"cell_type":"markdown","metadata":{"id":"W0VYvSGbTGe1"},"source":["The `vector` contains **two numbers** but only has a **single dimension**. The dimension of a tensor is often referred to as its **rank** or **order**. It indicates the **number of indices or axes** required to access individual elements within the tensor.\n","\n","Another important concept for tensors is their `shape` attribute. The shape tells us how the elements inside them are arranged. Let's check out the shape of `vector`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zREV1bDTGe2","outputId":"2a6e7ceb-7eb2-422b-b006-2c6e4825272f"},"outputs":[{"data":{"text/plain":["torch.Size([2])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Check shape of vector\n","vector.shape"]},{"cell_type":"markdown","metadata":{"id":"9aWKppNyTGe2"},"source":["The **shape** of a tensor describes the **number of elements along each of its dimensions or axes**. It specifies how many elements are present in each dimension.\n","\n","The above returns `torch.Size([2])` which means the vector has a shape of `[2]` in the single dimension."]},{"cell_type":"code","source":["scalar.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kcJr9jpjuUX","executionInfo":{"status":"ok","timestamp":1693721383142,"user_tz":-540,"elapsed":308,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"62de5ddc-9dd1-421b-9523-c1ddfd764cef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["#### **Matrix**"],"metadata":{"id":"aXBQV6Ggfdhu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5iNwCYL8QO9","outputId":"6c9c4df2-a8cd-490c-d57b-fe94d591716c","executionInfo":{"status":"ok","timestamp":1686102747576,"user_tz":-540,"elapsed":255,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 7,  8],\n","        [ 9, 10]])"]},"metadata":{},"execution_count":13}],"source":["# Matrix\n","MATRIX = torch.tensor([[7, 8],\n","                       [9, 10]])\n","MATRIX"]},{"cell_type":"markdown","metadata":{"id":"a3U1bCdjTGe3"},"source":["Matrices are as flexible as vectors, except they've got an **extra dimension**.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LREUbeb8r8j","outputId":"cdb0bfdf-860a-4ed4-9eb9-d3f738f0389f","executionInfo":{"status":"ok","timestamp":1686102749570,"user_tz":-540,"elapsed":248,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":14}],"source":["# Check number of dimensions\n","MATRIX.ndim"]},{"cell_type":"markdown","metadata":{"id":"LhXXgq-dTGe3"},"source":["`MATRIX` has **two dimensions**. What `shape` do you think it will have?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TL26I31TGe3","outputId":"cef3a941-caa4-4697-cd24-44c65c7583f3","executionInfo":{"status":"ok","timestamp":1686102750575,"user_tz":-540,"elapsed":2,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":15}],"source":["MATRIX.shape"]},{"cell_type":"markdown","metadata":{"id":"dvLpUvrKTGe4"},"source":["We get the output `torch.Size([2, 2])` because the `MATRIX` is two elements deep and two elements wide. How about we create a **tensor**?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEMDQr188QWW","outputId":"b9bd45bd-c07d-4efa-add9-65adb0e9e147","executionInfo":{"status":"ok","timestamp":1686103229005,"user_tz":-540,"elapsed":247,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1, 2, 3],\n","         [3, 6, 9],\n","         [2, 4, 5]]])"]},"metadata":{},"execution_count":19}],"source":["# Tensor\n","TENSOR = torch.tensor([[[1, 2, 3],\n","                        [3, 6, 9],\n","                        [2, 4, 5]]])\n","TENSOR"]},{"cell_type":"code","source":["TENSOR.ndim"],"metadata":{"id":"xUu6wazFix_U","executionInfo":{"status":"ok","timestamp":1686102601081,"user_tz":-540,"elapsed":2,"user":{"displayName":"","userId":""}},"outputId":"d43afb88-2014-421a-bb84-d95e386546e1","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["TENSOR.shape"],"metadata":{"id":"rmzv43YaiyGx","executionInfo":{"status":"ok","timestamp":1686102604354,"user_tz":-540,"elapsed":246,"user":{"displayName":"","userId":""}},"outputId":"d98d3392-4ba9-4049-cf60-8d2019c4d70f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 3])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"Ena08iG0lMOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z49bVb_JC4K1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"huKZ6QlYTGe7"},"source":["## **1.2. Tensor datatypes**\n","\n","There are many different tensor [datatypes](https://pytorch.org/docs/stable/tensors.html#data-types) available in PyTorch. Some are **specific for CPU** and some are **better for GPU** (generally if we see `torch.cuda` anywhere, the tensor is being used for GPU since Nvidia GPUs use a computing toolkit called CUDA).\n","\n","The most common (and default) type is `torch.float32` or `torch.float`. This is referred to as \"**32-bit floating point**\".\n","But there's also **16-bit floating point** (`torch.float16` or `torch.half`) and **64-bit floating point** (`torch.float64` or `torch.double`).\n","\n","There's also **8-bit**, **16-bit**, **32-bit** and **64-bit** integers.\n","An integer is a flat round number like `7` whereas a float has a decimal `7.0`.\n","The reason for all of these is to do with **precision**, amount of detail used to describe a number, in computing. The higher the precision value (8, 16, 32), the **more detail** used to express a number. So **lower precision datatypes are generally faster to compute** on but **sacrifice some performance on evaluation metrics** like accuracy (faster to compute but less accurate).\n","\n","Let's see how to create some tensors with specific datatypes. We can do so using the `dtype` parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3MoGnpw9XaF","outputId":"fe15677b-35ff-4b00-ae40-fdb514022d87","executionInfo":{"status":"ok","timestamp":1686104256512,"user_tz":-540,"elapsed":258,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3]), torch.float32, device(type='cpu'))"]},"metadata":{},"execution_count":30}],"source":["# Default datatype for tensors is float32\n","float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n","                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n","                               device=None, # defaults to None, which uses the default tensor type\n","                               requires_grad=False) # if True, track (record) gradients with this tensor operations performed\n","\n","float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"]},{"cell_type":"markdown","metadata":{"id":"MhP8kzDfe_ty"},"source":[" **Note**:\n"," > Aside from **shape** issues (tensor shapes don't match up), two of the other most common issues are **datatype** (eg, during dot product) and **device** issues. For example, one of tensors is `torch.float32` and the other is `torch.float16` (**PyTorch often likes tensors to be the *same* format**). Or one of your tensors is on the CPU and the other is on the GPU (**PyTorch likes calculations between tensors to be on the *same* device**).\n","\n","For now let's create a tensor with `dtype=torch.float16`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKSuajld_09s","outputId":"5e72d646-3fa1-4d8d-bd2d-115e8a36a7c7","executionInfo":{"status":"ok","timestamp":1686110094708,"user_tz":-540,"elapsed":396,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float16"]},"metadata":{},"execution_count":49}],"source":["float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n","                               dtype=torch.float16) # torch.half would also work\n","\n","float_16_tensor.dtype"]},{"cell_type":"code","source":["float_32_tensor2 = float_16_tensor.type(torch.float32)\n","\n","float_32_tensor2, float_32_tensor2.dtype"],"metadata":{"id":"omIM0onc_Mrp","executionInfo":{"status":"ok","timestamp":1686110120249,"user_tz":-540,"elapsed":265,"user":{"displayName":"","userId":""}},"outputId":"a2697821-07db-4a25-8ce7-e643d4872094","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3., 6., 9.]), torch.float32)"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"QBu33WihOXBk"},"source":["You can change the datatypes of tensors using [`torch.Tensor.type(dtype=None)`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html) where the `dtype` parameter is the datatype you'd like to use."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rY2FEsCAOaLu","outputId":"507f1ade-7c7a-4172-fa48-60c9ac4831c0"},"outputs":[{"data":{"text/plain":["torch.float32"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor and check its datatype\n","tensor = torch.arange(10., 100., 10.)\n","tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"jR30FHEc92of"},"source":["Now we'll create another tensor the same as before but change its datatype to `torch.float16`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cac8gRYjOeab","outputId":"96e5ce12-bc29-4a2b-f81c-bfc89ea2d075"},"outputs":[{"data":{"text/plain":["tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# Create a float16 tensor\n","tensor_float16 = tensor.type(torch.float16)\n","tensor_float16"]},{"cell_type":"markdown","metadata":{"id":"ndVlKJZ4-7_5"},"source":["And we can do something similar to make a `torch.int8` tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Yqovld2Oj6s","outputId":"667da17f-e38f-404a-bd2d-63683e45c99a"},"outputs":[{"data":{"text/plain":["tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# Create a int8 tensor\n","tensor_int8 = tensor.type(torch.int8)\n","tensor_int8"]},{"cell_type":"markdown","metadata":{"id":"44GxVabar-xe"},"source":["> **Note:** Different datatypes can be confusing to begin with. But, the l**ower the number** (e.g. 32, 16, 8), the **less precise** a computer stores the value. And with a lower amount of storage, this generally results in **faster computation** and a **smaller overall model**. Mobile-based neural networks often operate with **8-bit integers**, smaller and faster to run but less accurate than their `float32` counterparts. For more on this, I'd read up about [precision in computing](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n","\n","> **Exercise:** So far we've covered a fair few tensor methods but there's a bunch more in the [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) documentation. Spending some time in the documentation is recommended by scrolling through and looking into any that catch your eye."]},{"cell_type":"markdown","metadata":{"id":"gUjkB2AX7Upz"},"source":["## **1.3. Tensor Attributes**\n","\n","Once you've created tensors, you might want to get some information from them. We've seen these before but three of the most common attributes you'll want to find out about tensors are:\n","* `shape` - what shape is the tensor? (some operations require specific shape rules)\n","* `dtype` - what datatype are the elements within the tensor stored in?\n","* `device` - what device is the tensor stored on? (usually GPU or CPU)\n","\n","\n","Both **attributes and functions are used to represent properties and behaviors of objects**, but they have distinct characteristics and purposes:\n","\n","> **Attributes**: Attributes are variables that belong to an object or a class. They store information or state about the object or class. Attributes can be accessed and modified using **dot** notation (`object.attribute` or `class.attribute`). They are usually accessed directly **without parentheses**.\n","\n","> **Functions**: They define a set of instructions to be executed when called or invoked. Functions can take **input** parameters (arguments) and may **return** a value. They are invoked by using parentheses (`object.method()` or `function()`).\n","\n","Let's create a random tensor and find out details about it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hd_X4D0j7Umq","outputId":"a5a3aa37-38ea-4224-ba0a-f17a9b528c86","executionInfo":{"status":"ok","timestamp":1686111484783,"user_tz":-540,"elapsed":434,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.6748, 0.1052, 0.7866, 0.1365],\n","        [0.9178, 0.6855, 0.8798, 0.8456],\n","        [0.1475, 0.3835, 0.2620, 0.7551]])\n"]}],"source":["# Create a tensor\n","some_tensor = torch.rand(3, 4)\n","print(some_tensor)"]},{"cell_type":"code","source":["# Find out details about it\n","print(f\"Shape of tensor: {some_tensor.shape}\")\n","print(f\"Datatype of tensor: {some_tensor.dtype}\")\n","print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU"],"metadata":{"id":"lIoSFptbESn7","executionInfo":{"status":"ok","timestamp":1686111485823,"user_tz":-540,"elapsed":2,"user":{"displayName":"","userId":""}},"outputId":"d80b5e39-4dac-4b3e-89b8-89ace006742e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}]},{"cell_type":"markdown","metadata":{"id":"45K-E5uPg6cj"},"source":["> **Note:** When you run into issues in PyTorch, it's very often one to do with one of the three attributes above:\n","  * \"*what shape are my tensors? what datatype are they and where are they stored?*\""]},{"cell_type":"markdown","metadata":{"id":"nEPqVL7fTGfC"},"source":["## **1.4. Indexing**\n","\n","If you've ever done indexing on Python lists or NumPy arrays, indexing in PyTorch with tensors is very similar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSXzdxCQTGfD","outputId":"811c6326-c055-4e63-c44c-d6c3d53fabc8","executionInfo":{"status":"ok","timestamp":1686272145656,"user_tz":-540,"elapsed":250,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[1, 2, 3],\n","          [4, 5, 6],\n","          [7, 8, 9]]]),\n"," torch.Size([1, 3, 3]))"]},"metadata":{},"execution_count":20}],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1, 10).reshape(1, 3, 3)\n","x, x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dhuEsjS8QcT","outputId":"83f40c9d-3090-4540-9bc8-4a824eac1d14","executionInfo":{"status":"ok","timestamp":1686272150292,"user_tz":-540,"elapsed":276,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":21}],"source":["# Check number of dimensions\n","x.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdVv4iNRTGe5","outputId":"c75cdae7-8928-41cc-fcd4-89fe36bd2cd5","executionInfo":{"status":"ok","timestamp":1686272161227,"user_tz":-540,"elapsed":256,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 3])"]},"metadata":{},"execution_count":23}],"source":["# Check also the shape\n","x.shape"]},{"cell_type":"code","source":["x[0]"],"metadata":{"id":"MEU28hsKjAFc","executionInfo":{"status":"ok","timestamp":1686272156313,"user_tz":-540,"elapsed":274,"user":{"displayName":"","userId":""}},"outputId":"63df89ca-eb8a-4b51-c6ba-03fb530046a8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"zxk8GU7oTGe5"},"source":["Alright, it outputs `torch.Size([1, 3, 3])`.\n","The dimensions go outer to inner.\n","That means there's 1 dimension of 3 by 3.\n","\n","![example of different tensor dimensions](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n","\n","> **Note:** You might've noticed me using lowercase letters for `scalar` and `vector` and uppercase letters for `MATRIX` and `TENSOR`. This was on purpose. In practice, you'll often see scalars and vectors denoted as lowercase letters such as `y` or `a`. And matrices and tensors denoted as uppercase letters such as `X` or `W`.\n",">\n","> You also might notice the names martrix and tensor used interchangably. This is common. Since in PyTorch you're often dealing with `torch.Tensor`s (hence the tensor name), however, the shape and dimensions of what's inside will dictate what it actually is.\n","\n","Let's summarise.\n","\n","| Name | What is it? | Number of dimensions | Lower or upper (usually/example) |\n","| ----- | ----- | ----- | ----- |\n","| **scalar** | a single number | 0 | Lower (`a`) |\n","| **vector** | a set of numbers in a single dimension| 1 | Lower (`y`) |\n","| **matrix** | a 2-dimensional array of numbers | 2 | Upper (`Q`) |\n","| **tensor** | an n-dimensional array of numbers | n | Upper (`X`) |\n","\n","![scalar vector matrix tensor and what they look like](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)"]},{"cell_type":"markdown","metadata":{"id":"xQG5krnKG43B"},"source":["Indexing values goes outer dimension -> inner dimension (check out the square brackets)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zv_Z3IAzTGfD","outputId":"cd04efe7-72f9-4c48-a5bb-28c38a84cabf","executionInfo":{"status":"ok","timestamp":1686272346588,"user_tz":-540,"elapsed":267,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The given tensor is: \n","tensor([[[1, 2, 3],\n","         [4, 5, 6],\n","         [7, 8, 9]]])\n","First square bracket:\n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","Second square bracket: tensor([1, 2, 3])\n","Third square bracket: 1\n","Third square bracket second item: 2\n"]}],"source":["# Let's index bracket by bracket\n","print(f\"The given tensor is: \\n{x}\")\n","print(f\"First square bracket:\\n{x[0]}\") # same as x[0,:,:]\n","print(f\"Second square bracket: {x[0][0]}\") # same as x[0,0], x[0,0,:]\n","print(f\"Third square bracket: {x[0][0][0]}\") # same as x[0,0,0]\n","print(f\"Third square bracket second item: {x[0][0][1]}\") # same as x[0,0,1]"]},{"cell_type":"markdown","metadata":{"id":"XaLjaIFxHe89"},"source":["You can also use `:` to specify \"**all values in this dimension**\" and then use a comma (`,`) to add another dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCT09pqeTGfD","outputId":"bfdbbd9b-d189-495a-e729-48d409da125e","executionInfo":{"status":"ok","timestamp":1686271823254,"user_tz":-540,"elapsed":256,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3]])"]},"metadata":{},"execution_count":15}],"source":["# Get all values of 0th dimension and the 0 index of 1st dimension\n","x[:, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwDx_gMsTGfD","outputId":"13cd4f2b-fa3e-45e1-cd50-d1aea9e1b308","executionInfo":{"status":"ok","timestamp":1686271840927,"user_tz":-540,"elapsed":271,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2, 5, 8]])"]},"metadata":{},"execution_count":16}],"source":["# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n","x[:, :, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiw3_1E3TGfD","outputId":"bf5ee65e-6934-4a4e-cb5c-cdd9721131dc","executionInfo":{"status":"ok","timestamp":1686271845917,"user_tz":-540,"elapsed":401,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5])"]},"metadata":{},"execution_count":17}],"source":["# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n","x[:, 1, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFVEgrKhTGfD","outputId":"2d393b1f-ad69-42fe-cb10-14d8d5a76fd2","executionInfo":{"status":"ok","timestamp":1686271847559,"user_tz":-540,"elapsed":2,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":18}],"source":["# Get index 0 of 0th and 1st dimension and all values of 2nd dimension\n","x[0, 0, :] # same as x[0][0]"]},{"cell_type":"markdown","metadata":{"id":"6Ik0r11RIxtm"},"source":["Indexing can be quite confusing to begin with, especially with larger tensors (I still have to try indexing multiple times to get it right)."]},{"cell_type":"code","source":["# Slicing\n","x = torch.rand(5,3)\n","print(x)\n","print(x[:, 0]) # all rows, column 0\n","print(x[1, :]) # row 1, all columns\n","print(x[1,1]) # element at 1, 1"],"metadata":{"id":"VMKuu81RA_Q6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dms7G4nkTGe5"},"source":["## **1.5. Random tensors**\n","\n","A machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.\n","\n","In essence:\n","\n","`Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers...`\n","\n","As a data scientist, we can define how the machine learning model starts (**initialization**), looks at data (**representation**) and updates (**optimization**) its random numbers.\n","\n","For now, let's see how to create a tensor of random numbers using [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html) and passing in the `size` parameter."]},{"cell_type":"code","source":["x = torch.rand(5,3) # random uniform in the range [0, 1)]\n","print(x)\n","x = torch.randn(5,3) # random normal\n","print(x)"],"metadata":{"id":"izwNNMbIBJuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOJEtDx--GnK","outputId":"2b090cfb-f2ba-4f4c-b856-67ac937de702","executionInfo":{"status":"ok","timestamp":1686105733621,"user_tz":-540,"elapsed":233,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.5239, 0.8177, 0.3471, 0.6224],\n","         [0.5464, 0.7673, 0.1950, 0.1994],\n","         [0.2721, 0.9299, 0.0354, 0.6212]]),\n"," torch.float32)"]},"metadata":{},"execution_count":34}],"source":["# Create a random tensor of size (3, 4)\n","random_tensor = torch.rand(size=(3, 4))\n","random_tensor, random_tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"-wB1c_cXTGe5"},"source":["The flexibility of `torch.rand()` is that we can adjust the `size` to be whatever we want. For example, say you wanted a random tensor in the common image shape of `[224, 224, 3]` (`[height, width, color_channels`])."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMF_NUp3Ym__","outputId":"e38c3e09-9f19-4b47-ece3-d9e6d7cf233b","executionInfo":{"status":"ok","timestamp":1686103675929,"user_tz":-540,"elapsed":270,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([224, 224, 3]), 3)"]},"metadata":{},"execution_count":28}],"source":["# Create a random tensor of size (224, 224, 3)\n","random_image_size_tensor = torch.rand(size=(224, 224, 3))\n","random_image_size_tensor.shape, random_image_size_tensor.ndim"]},{"cell_type":"markdown","source":["**Uninitialized Tensors (`torch.empty(size)`)**"],"metadata":{"id":"A-uGFrmUZg93"}},{"cell_type":"code","source":["x = torch.empty(1) # scalar\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6BDI3WUPMq3","executionInfo":{"status":"ok","timestamp":1653619462783,"user_tz":-540,"elapsed":252,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"357b061b-5991-4696-cc63-dfda0d6f9559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([4.6975e-35])\n"]}]},{"cell_type":"code","source":["x = torch.empty(3) # vector, 1D\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BCp8-IMPj3q","executionInfo":{"status":"ok","timestamp":1653620063629,"user_tz":-540,"elapsed":7,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"b11ec068-3872-4015-d773-97f10ca98b2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([4.6977e-35, 0.0000e+00, 1.5975e-43])\n"]}]},{"cell_type":"code","source":["x = torch.empty(2,3) # matrix, 2D\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-tTQ_5cP0ba","executionInfo":{"status":"ok","timestamp":1653620339026,"user_tz":-540,"elapsed":277,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"80c5997a-e9a1-40e3-f634-fef07f91da0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[4.6977e-35, 0.0000e+00, 1.5975e-43],\n","        [1.3873e-43, 1.4574e-43, 6.4460e-44]])\n"]}]},{"cell_type":"code","source":["x = torch.empty(2,2,3) # tensor, 3 dimensions\n","#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2qgwNCHP0ji","executionInfo":{"status":"ok","timestamp":1653620402835,"user_tz":-540,"elapsed":271,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"1208dec1-27d5-4af1-f5cc-0a69be0f659d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[4.6976e-35, 0.0000e+00, 2.3694e-38],\n","         [2.3694e-38, 2.3694e-38, 2.3694e-38]],\n","\n","        [[3.7293e-08, 1.4838e-41, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"7gU3ubCrUkI-"},"source":["## **1.6. Reproducibility**\n","\n","Neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things) to better describe patterns in data.\n","\n","In short:\n","\n","**start with random numbers -> tensor operations -> try to make better (again and again and again)**\n","\n","Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.\n","Why?\n","You can get the same (or very similar) results on your computer running the same code repeatable experiments. That's where **reproducibility** comes in.\n","\n","Let's see a brief example of reproducibility in PyTorch. We'll start by creating two random tensors, since they're random, you'd expect them to be different right?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSwxnwEbTGfF","outputId":"73b34154-734f-496f-9b55-b6aaa137e854"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor A:\n","tensor([[0.8016, 0.3649, 0.6286, 0.9663],\n","        [0.7687, 0.4566, 0.5745, 0.9200],\n","        [0.3230, 0.8613, 0.0919, 0.3102]])\n","\n","Tensor B:\n","tensor([[0.9536, 0.6002, 0.0351, 0.6826],\n","        [0.3743, 0.5220, 0.1336, 0.9666],\n","        [0.9754, 0.8474, 0.8988, 0.1105]])\n","\n","Does Tensor A equal Tensor B? (anywhere)\n"]},{"data":{"text/plain":["tensor([[False, False, False, False],\n","        [False, False, False, False],\n","        [False, False, False, False]])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","# Create two random tensors\n","random_tensor_A = torch.rand(3, 4)\n","random_tensor_B = torch.rand(3, 4)\n","\n","print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n","print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n","print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n","random_tensor_A == random_tensor_B"]},{"cell_type":"markdown","metadata":{"id":"nPU6mDKJnr8M"},"source":["Just as you might've expected, the tensors come out with different values.\n","But what if you wanted to created two random tensors with the *same* values.\n","As in, the tensors would still contain random values but they would be of the same flavour.\n","That's where [`torch.manual_seed(seed)`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) comes in, where `seed` is an integer (like `42` but it could be anything) that flavours the randomness.\n","Let's try it out by creating some more *flavoured* random tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sB6d1GfYTGfF","outputId":"883968fa-ad5d-47a5-eaff-c77299cb6e3b","executionInfo":{"status":"ok","timestamp":1686275208682,"user_tz":-540,"elapsed":264,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor C:\n","tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n","        [0.3904, 0.6009, 0.2566, 0.7936],\n","        [0.9408, 0.1332, 0.9346, 0.5936]])\n","\n","Tensor D:\n","tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n","        [0.3904, 0.6009, 0.2566, 0.7936],\n","        [0.9408, 0.1332, 0.9346, 0.5936]])\n","\n","Does Tensor C equal Tensor D? (anywhere)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True]])"]},"metadata":{},"execution_count":33}],"source":["import torch\n","import random\n","\n","# # Set the random seed\n","RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n","torch.manual_seed(seed=RANDOM_SEED)\n","random_tensor_C = torch.rand(3, 4)\n","\n","# Have to reset the seed every time a new rand() is called\n","# Without this, tensor_D would be different to tensor_C\n","torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n","random_tensor_D = torch.rand(3, 4)\n","\n","print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n","print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n","print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n","random_tensor_C == random_tensor_D"]},{"cell_type":"markdown","metadata":{"id":"uct53Xr5QRC_"},"source":["It looks like setting the seed worked.\n","\n","Checkout more on the PyTorch reproducibility : [documentation](https://pytorch.org/docs/stable/notes/randomness.html)."]},{"cell_type":"markdown","metadata":{"id":"0MQNTY0eTGe6"},"source":["## **1.7. Zeros and ones**\n","\n","Sometimes you'll just want to fill tensors with zeros or ones.\n","This happens a lot with masking (like masking some of the values in one tensor with zeros to let a model know not to learn them). Let's create a tensor full of zeros with [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html) Again, the `size` parameter comes into play."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCzhd0hl9Vp6","outputId":"f26a54c7-daf0-47d9-a72f-9fe3b0038282","executionInfo":{"status":"ok","timestamp":1686105737086,"user_tz":-540,"elapsed":267,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]),\n"," torch.float32)"]},"metadata":{},"execution_count":35}],"source":["# Create a tensor of all zeros\n","zeros = torch.zeros(size=(3, 4))\n","zeros, zeros.dtype"]},{"cell_type":"code","source":["random_tensor * zeros"],"metadata":{"id":"5-MyZwkvuz68","executionInfo":{"status":"ok","timestamp":1686106952351,"user_tz":-540,"elapsed":266,"user":{"displayName":"","userId":""}},"outputId":"69889cf3-7788-4cf9-8cec-67c83037aa26","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"WDQBZJRUZWTN"},"source":["We can do the same to create a tensor of all ones except using [`torch.ones()` ](https://pytorch.org/docs/stable/generated/torch.ones.html) instead."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRe6sSXiTGe6","outputId":"3f45b0b8-7f65-423d-c664-f5b5f7866fd2"},"outputs":[{"data":{"text/plain":["(tensor([[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]),\n"," torch.float32)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor of all ones\n","ones = torch.ones(size=(3, 4))\n","ones, ones.dtype"]},{"cell_type":"code","source":["x = torch.zeros(2, 3) # fill with 0\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4qpPBSxP0pS","executionInfo":{"status":"ok","timestamp":1653618069248,"user_tz":-540,"elapsed":252,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"8f497ce6-c048-428d-ae58-c716ec1c0817"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}]},{"cell_type":"code","source":["x = torch.ones(2, 3) # fill with 1\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxYJn6vWP0sB","executionInfo":{"status":"ok","timestamp":1653618092502,"user_tz":-540,"elapsed":266,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"31a9946d-949a-4682-8203-46d24a917568"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n"]}]},{"cell_type":"code","source":["x = torch.ones(2, 3)\n","print(x.dtype) # check data type"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id1va3WCP0vS","executionInfo":{"status":"ok","timestamp":1653618133746,"user_tz":-540,"elapsed":284,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"726fe0e4-c139-4435-d30c-8a678d208de3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float32\n"]}]},{"cell_type":"code","source":["x = torch.ones(2, 3, dtype=torch.int)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVdqApqqP0yB","executionInfo":{"status":"ok","timestamp":1653618171790,"user_tz":-540,"elapsed":252,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"df7bd3d4-f8a9-48a1-a6be-1ee11ff31a01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int32\n"]}]},{"cell_type":"code","source":["x = torch.ones(2, 3, dtype=torch.double)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJ0y6L2mP00x","executionInfo":{"status":"ok","timestamp":1653618195476,"user_tz":-540,"elapsed":272,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"ca257805-834f-42e6-d6d3-f09d9d67f590"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float64\n"]}]},{"cell_type":"code","source":["x = torch.ones(2, 3, dtype=torch.float16)\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQzGv07BP03y","executionInfo":{"status":"ok","timestamp":1653618216082,"user_tz":-540,"elapsed":269,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"7b51cb34-e87a-4f6d-8fbc-f63160f277df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float16\n"]}]},{"cell_type":"code","source":["x = torch.ones(2, 3, dtype=torch.float16)\n","print(x.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lb9ThaOEP06i","executionInfo":{"status":"ok","timestamp":1653618254579,"user_tz":-540,"elapsed":265,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"06a70501-d9c8-4efb-88ae-730e562e2041"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3])\n"]}]},{"cell_type":"markdown","metadata":{"id":"hib1NYrSarL2"},"source":["## **1.8. Creating a range and tensors like**\n","\n","Sometimes you might want a range of numbers, such as 1 to 10 or 0 to 100. You can use `torch.arange(start, end, step)` to do so, where:\n","* `start` = start of range (e.g. 0)\n","* `end` = end of range (e.g. 10)\n","* `step` = how many steps in between each value (e.g. 1)\n","\n","> **Note:** In Python, you can use `range()` to create a range. However in PyTorch, `torch.range()` is deprecated and may show an error in the future."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IqUs81d9W4W","outputId":"f810587f-1980-42ed-af83-d8eabacb373f","executionInfo":{"status":"ok","timestamp":1686103859457,"user_tz":-540,"elapsed":316,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-a09072c806d9>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{},"execution_count":29}],"source":["# Use torch.arange(), torch.range() is deprecated\n","zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n","\n","# Create a range of values 0 to 10\n","zero_to_ten = torch.arange(start=0, end=10, step=1)\n","zero_to_ten"]},{"cell_type":"markdown","metadata":{"id":"i-bXf0Ugbh-D"},"source":["Sometimes you might want one tensor of a certain type with the same shape as another tensor. For example, a tensor of all zeros with the same shape as a previous tensor. To do so you can use [`torch.zeros_like(input)`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) or [`torch.ones_like(input)`](https://pytorch.org/docs/1.9.1/generated/torch.ones_like.html) which return a tensor filled with zeros or ones in the same shape as the `input` respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvXwUut5BhHq","outputId":"096b2f8e-8c21-4ace-97b9-c36b92b2fe77"},"outputs":[{"data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Can also create a tensor of zeros similar to another tensor\n","ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n","ten_zeros"]},{"cell_type":"markdown","source":["**Iterable vs Iterator**\n","\n","\n","> **Note**: In Python, an iterable and an iterator are related concepts but have distinct roles:\n","* An **iterable** is any object that can be looped over or iterated. It is a more general concept. Examples of iterables include **lists**, **tuples**, **strings**, **dictionaries**, and **sets**.\n","* An **iterator** is an object that represents a stream of data and provides a way to access elements sequentially. It implements two essential methods: `iter()` returns the iterator object itself and `next()` returns the next element from the stream. When there are no more elements, `next()` raises the `StopIteration` exception. Iterators maintain the state and remember the position in the iteration. Iterators are typically used in `for` loops or when you explicitly want to iterate over a sequence of items one by one."],"metadata":{"id":"S_mZXOWp6XhX"}},{"cell_type":"code","source":["my_list = [1, 2, 3]\n","print(f\"list: {my_list}\")\n","print(f\"Iterable:\")\n","for item in my_list:\n","    print(item)"],"metadata":{"id":"DYqmm4CS7a6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iter_obj = iter(my_list)\n","print(f\"list: {iter_obj}\")\n","print(next(iter_obj))\n","print(next(iter_obj))\n","print(next(iter_obj))\n","print(next(iter_obj))"],"metadata":{"id":"aCHnO0VC78NY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uQILXR05c_4z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8ZaW0Bq7rCm"},"source":["# **2. PyTorch Tensors and NumPy Arrays**\n","\n","Since NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.\n","The two main methods you'll want to use for NumPy to PyTorch (and back again) are:\n","* [`torch.from_numpy(ndarray)`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - NumPy array -> PyTorch tensor.\n","* [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) - PyTorch tensor -> NumPy array.\n","\n","Let's try them out."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDrDCnvY7rKS","outputId":"8ee77958-3e84-45fb-d6ec-f4d171bf3e98","executionInfo":{"status":"ok","timestamp":1686272541313,"user_tz":-540,"elapsed":267,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1., 2., 3., 4., 5., 6., 7.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"metadata":{},"execution_count":29}],"source":["# NumPy array to tensor\n","import torch\n","import numpy as np\n","array = np.arange(1.0, 8.0)\n","tensor = torch.from_numpy(array)\n","array, tensor"]},{"cell_type":"markdown","metadata":{"id":"16JG6cONLPnO"},"source":["**Note:**\n","> By default, NumPy arrays are created with the datatype `float64` and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above). However, many PyTorch calculations default to using `float32`.\n","So if you want to convert your **NumPy array (float64) -> PyTorch tensor (float64) -> PyTorch tensor (float32)**, you can use\n","* `tensor = torch.from_numpy(array).type(torch.float32)`."]},{"cell_type":"code","source":["array.dtype, tensor.dtype"],"metadata":{"id":"CrJu3FzysEcZ","executionInfo":{"status":"ok","timestamp":1686272790659,"user_tz":-540,"elapsed":350,"user":{"displayName":"","userId":""}},"outputId":"aed3e9a8-1dae-473a-ca73-e95d70c968cb","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(dtype('float64'), torch.float64)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["But `torch.float64` is **not the default datatype**. It is `torch.float32`. Eg:"],"metadata":{"id":"WtFXorUtsTdN"}},{"cell_type":"code","source":["torch.arange(1.0,8.0).dtype"],"metadata":{"id":"tD00FbOJsWZv","executionInfo":{"status":"ok","timestamp":1686272935331,"user_tz":-540,"elapsed":247,"user":{"displayName":"","userId":""}},"outputId":"8afa4437-a538-477b-f408-154440fc3110","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["Because we reassigned `tensor` above, if you change the tensor, the array stays the same."],"metadata":{"id":"dIaGOvyusEoM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovwl7VCREv8L","outputId":"efd21eb9-0010-436a-dc29-f851e3d7d77a"},"outputs":[{"data":{"text/plain":["(array([2., 3., 4., 5., 6., 7., 8.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["# Change the array, keep the tensor\n","array = array + 1\n","array, tensor"]},{"cell_type":"markdown","metadata":{"id":"geVvu1p0MTWc"},"source":["And if you want to go from PyTorch tensor to NumPy array, you can call `tensor.numpy()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xw_7ZyVaTKxQ","outputId":"54d6f347-d3f6-44df-9155-83d980c31780"},"outputs":[{"data":{"text/plain":["(tensor([1., 1., 1., 1., 1., 1., 1.]),\n"," array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# Tensor to NumPy array\n","tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n","numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n","tensor, numpy_tensor"]},{"cell_type":"markdown","metadata":{"id":"Dt8yEV1jMfi2"},"source":["And the same rule applies as above, if you change the original `tensor`, the new `numpy_tensor` stays the same."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMp6ZSkET4_Y","outputId":"100678a4-c220-4a44-e4a5-0542359cb9de"},"outputs":[{"data":{"text/plain":["(tensor([2., 2., 2., 2., 2., 2., 2.]),\n"," array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["# Change the tensor, keep the array the same\n","tensor = tensor + 1\n","tensor, numpy_tensor"]},{"cell_type":"code","source":["# Numpy\n","# Converting a Torch Tensor to a NumPy array and vice versa is very easy\n","a = torch.ones(5)\n","print(a)\n","# torch to numpy with .numpy()\n","b = a.numpy()\n","print(b)\n","print(type(b))"],"metadata":{"id":"1ykwmWIDCPXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Careful**:\n","> If the Tensor is on the CPU (not the GPU), both objects will share the same memory location, so **changing one will also change the other**."],"metadata":{"id":"Ygv3gY-hCbXr"}},{"cell_type":"code","source":["a.add_(1)\n","print(a)\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBvhl9BSVdDV","executionInfo":{"status":"ok","timestamp":1653700087962,"user_tz":-540,"elapsed":282,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"3d168194-5b65-440f-9a2c-e0023bba524e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 2., 2., 2., 2.])\n","[2. 2. 2. 2. 2.]\n"]}]},{"cell_type":"code","source":["# numpy to torch with .from_numpy(x)\n","import numpy as np\n","a = np.ones(5)\n","b = torch.from_numpy(a)\n","print(a)\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9VkxOcbVdGW","executionInfo":{"status":"ok","timestamp":1653700168197,"user_tz":-540,"elapsed":292,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"4d31dddb-842f-447a-d71e-e6377869cb7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1. 1. 1.]\n","tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"]}]},{"cell_type":"code","source":["# again be careful when modifying\n","a += 1\n","print(a)\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XH1BlAD7Nlk2","executionInfo":{"status":"ok","timestamp":1653700200162,"user_tz":-540,"elapsed":310,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"45a93fb8-635c-4e0d-9761-6960945c0250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2. 2. 2. 2. 2.]\n","tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BdiWvoAi7UjL"},"source":["# **3. Manipulating tensors (tensor operations)**\n","\n","In deep learning, data (**images**, **text**, **video**, **audio**, **protein structures**, etc) gets represented as tensors. A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a **representation of the patterns** in the input data.\n","\n","\n","To perform operations on tensors in PyTorch, there are key points to be considered:\n","\n","> **Compatible shapes**: For element-wise operations (e.g., addition, subtraction, multiplication), the tensors involved in the operation must have the **same shape** or be broadcastable to the **same shape**.\n","\n","> **Data type compatibility**: Tensors involved in operations should have **compatible data types**. Mixing incompatible data types in operations can lead to **errors** or **unexpected behavior**.\n","\n","> **Device compatibility**: Tensors must reside on the **same device** (either CPU or GPU) in order to perform operations together. PyTorch provides mechanisms to move tensors between devices using methods like `.to()` or `.cuda()`."]},{"cell_type":"markdown","metadata":{"id":"Sk_6Dd7L7Uce"},"source":["## **3.1. Basic operations**\n","\n","Let's start with a few of the fundamental operations, addition (`+`), subtraction (`-`), multiplication (`*`). They work just as you think they would."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X71WpQoPD7a4","outputId":"70ddb9f1-69ca-4519-a3e5-0c56feb6ac6d","executionInfo":{"status":"ok","timestamp":1686111964491,"user_tz":-540,"elapsed":304,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([11, 12, 13])"]},"metadata":{},"execution_count":69}],"source":["# Create a tensor of values and add a number to it\n","tensor = torch.tensor([1, 2, 3])\n","tensor + 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp4TlTWWEFeO","outputId":"8f3484f1-6843-4964-cd74-de76f853f337","executionInfo":{"status":"ok","timestamp":1686111968410,"user_tz":-540,"elapsed":302,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30])"]},"metadata":{},"execution_count":70}],"source":["# Multiply it by 10\n","tensor * 10"]},{"cell_type":"markdown","metadata":{"id":"-1VEHnuRkn8Q"},"source":["Notice how the tensor values above didn't end up being `tensor([110, 120, 130])`, this is because the *values inside the tensor don't change unless they're reassigned*."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuB1UjCIEJIA","outputId":"57cae862-c145-4681-d74b-fe6d77f2125a"},"outputs":[{"data":{"text/plain":["tensor([1, 2, 3])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Tensors don't change unless reassigned\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"VYvqGpUTk1o6"},"source":["Let's subtract a number and this time we'll reassign the `tensor` variable."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4iWKoLsENry","outputId":"b985fb3e-859f-478f-f42d-547acccca977","executionInfo":{"status":"ok","timestamp":1686112037407,"user_tz":-540,"elapsed":320,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-9, -8, -7])"]},"metadata":{},"execution_count":71}],"source":["# Subtract and reassign\n","tensor = tensor - 10\n","tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFgZY-PaFNXa","outputId":"c4ea6409-f16c-4134-a123-aa247f24efb7","executionInfo":{"status":"ok","timestamp":1686112049212,"user_tz":-540,"elapsed":319,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":72}],"source":["# Add and reassign\n","tensor = tensor + 10\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"CYXDoIOzk-6I"},"source":["PyTorch also has a bunch of built-in functions like [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul) (short for multiplication) and [`torch.add()`](https://pytorch.org/docs/stable/generated/torch.add.html) to perform basic operations."]},{"cell_type":"code","source":["x = torch.rand(2, 2)\n","y = torch.rand(2, 2)\n","print(x)\n","print(y)"],"metadata":{"id":"aOxSs1KjAFbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z = x + y # elementwise addition\n","print(z)\n","z=torch.add(x,y)\n","print(z)"],"metadata":{"id":"s3mT0Rw3AFkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# substraction\n","z = x - y\n","print(z)\n","z = torch.sub(x, y)\n","print(z)"],"metadata":{"id":"7DmPbHeGAVbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# multiplication\n","z = x * y\n","print(z)\n","z = torch.mul(x,y)\n","print(z)"],"metadata":{"id":"KtW3Z3s3AZK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# division\n","z = x / y\n","print(z)\n","z = torch.div(x,y)\n","print(z)"],"metadata":{"id":"46MW6v_jAbiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVysdk3kFWbY","outputId":"f2021843-27bb-419c-d98e-4c03da591ac2","executionInfo":{"status":"ok","timestamp":1686112061144,"user_tz":-540,"elapsed":304,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([10, 20, 30])"]},"metadata":{},"execution_count":73}],"source":["# Can also use torch functions\n","torch.multiply(tensor, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxuPJIpNFbqO","outputId":"2333d287-1be8-496b-ee5c-bb4159643a14","executionInfo":{"status":"ok","timestamp":1686112072157,"user_tz":-540,"elapsed":309,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":74}],"source":["# Original tensor is still unchanged\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"70UNL33AlVQq"},"source":["However, it's more common to use the operator symbols like `*` instead of `torch.mul()`"]},{"cell_type":"markdown","source":["## **3.2. In-Place Operations**\n","\n","PyTorch supports in-place operations that **modify the tensor in-place**, indicated by a trailing **underscore** (e.g., `x.add_(y)`)."],"metadata":{"id":"17JTlf8CfXxx"}},{"cell_type":"code","source":["x = torch.rand(2, 2)\n","y = torch.rand(2, 2)\n","print(x)\n","print(y)"],"metadata":{"id":"vDgSHBq1_tUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note**:\n","\n","> In-place operations have some restrictions and may **not** be allowed in certain situations, such as when the tensor is involved in **autograd (automatic differentiation)** or when the tensor is **part of a computational graph**."],"metadata":{"id":"OEeS72tcOKyi"}},{"cell_type":"markdown","metadata":{"id":"TT5fVuyu7q5z"},"source":["## **3.3. Matrix Multiplication**\n","PyTorch implements matrix multiplication functionality in the [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) method. The main two points for matrix multiplication to remember are:\n","1. The **inner dimensions must match**:\n","  * `(3, 2) @ (3, 2)` won't work\n","  * `(2, 3) @ (3, 2)` will work\n","  * `(3, 2) @ (2, 3)` will work\n","2. The **resulting matrix has the shape of the outer dimensions**:\n"," * `(2, 3) @ (3, 2)` -> `(2, 2)`\n"," * `(3, 2) @ (2, 3)` -> `(3, 3)`\n","\n","> **Note:** \"`@`\" in Python is the symbol for matrix multiplication. Rules for matrix multiplication using [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) in the PyTorch documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZE7loucmDlEM","outputId":"6d8b1a68-06d2-4a31-88c0-78f58c3c844c","executionInfo":{"status":"ok","timestamp":1686127050334,"user_tz":-540,"elapsed":5101,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3])"]},"metadata":{},"execution_count":3}],"source":["import torch\n","tensor = torch.tensor([1, 2, 3])\n","tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvCBiiTTDk8y","outputId":"2173de40-4850-45db-c83c-7ac98b13d762","executionInfo":{"status":"ok","timestamp":1686119314902,"user_tz":-540,"elapsed":254,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":86}],"source":["# Matrix multiplication\n","torch.matmul(tensor, tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4E_pROBDk2r","outputId":"4c8850a6-528c-4a9f-d3cd-c821bb157900","executionInfo":{"status":"ok","timestamp":1686119318527,"user_tz":-540,"elapsed":458,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":87}],"source":["# Can also use the \"@\" symbol for matrix multiplication, though not recommended\n","tensor @ tensor"]},{"cell_type":"code","source":["# Shapes need to be in the right way\n","A = torch.tensor([[1, 2, 3],\n","                  [4, 5, 6]], dtype=torch.float32)\n","\n","B = torch.tensor([[7, 10],\n","                  [8, 11],\n","                  [9, 12]], dtype=torch.float32)\n","\n","torch.matmul(A, B)"],"metadata":{"id":"yyzIO8NLiw7U","executionInfo":{"status":"ok","timestamp":1686119478819,"user_tz":-540,"elapsed":315,"user":{"displayName":"","userId":""}},"outputId":"efa99cfd-b3ed-438b-c9c3-7faf84bcdcf6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 50.,  68.],\n","        [122., 167.]])"]},"metadata":{},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"obbginUMv43A"},"source":["You can do matrix multiplication by hand but it's not recommended. The in-built `torch.matmul()` method is faster.\n","The `%%time` magic command is used to measure and display the execution time of a specific code cell or block."]},{"cell_type":"code","source":["tensor = torch.tensor([1, 2, 3])"],"metadata":{"id":"nHaXz-B9j6vM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qMSaLOoJscL","outputId":"a785dad1-8de6-4a07-a1b4-5a3783fe5f88","executionInfo":{"status":"ok","timestamp":1686119634063,"user_tz":-540,"elapsed":6,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 392 s, sys: 0 ns, total: 392 s\n","Wall time: 401 s\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":91}],"source":["%%time\n","# Matrix multiplication by hand\n","# (avoid doing operations with for loops at all cost, they are computationally expensive)\n","value = 0\n","for i in range(len(tensor)):\n","  value += tensor[i] * tensor[i]\n","value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVWiKB0KwH74","outputId":"79afa273-2d53-482f-a7ea-5e7658328543","executionInfo":{"status":"ok","timestamp":1686119637233,"user_tz":-540,"elapsed":257,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 150 s, sys: 19 s, total: 169 s\n","Wall time: 174 s\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":92}],"source":["%%time\n","torch.matmul(tensor, tensor)"]},{"cell_type":"markdown","source":["Manual:\n","\n","  CPU times: user 762 s, sys: 1.03 ms, total: 1.8 ms\n","\n","  Wall time: 1.67 ms (**1.67 milli seconds**)\n","\n","  tensor(14)\n","\n","Built-in:\n","  CPU times: user 103 s, sys: 0 ns, total: 103 s\n","\n","  Wall time: 107 s (**107 micro seconds**)\n","\n","  tensor(14)\n"],"metadata":{"id":"-EjapVmlKw3S"}},{"cell_type":"markdown","source":["## **3.4. Element-Wise Multiplication**"],"metadata":{"id":"eCWamAfE_5lH"}},{"cell_type":"code","source":["# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n","tensor = torch.tensor([1, 2, 3])\n","print(tensor, \"*\", tensor)\n","print(\"Equals:\", tensor * tensor)"],"metadata":{"id":"eVlezP64ADUg","executionInfo":{"status":"ok","timestamp":1686127050335,"user_tz":-540,"elapsed":6,"user":{"displayName":"","userId":""}},"outputId":"97ebae53-9d32-48e9-b703-3638d3f80f05","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) * tensor([1, 2, 3])\n","Equals: tensor([1, 4, 9])\n"]}]},{"cell_type":"code","source":["X = torch.tensor([[1, 2, 3],\n","                  [4, 5, 6]], dtype=torch.float32)\n","Y = torch.tensor([[7, 8, 9],\n","                  [10, 11, 12]], dtype=torch.float32)\n","X * Y"],"metadata":{"id":"T7MaRjEqAQYy","executionInfo":{"status":"ok","timestamp":1686127132365,"user_tz":-540,"elapsed":4,"user":{"displayName":"","userId":""}},"outputId":"798e50da-0347-48d9-f22f-8c3f4d7d922a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 7., 16., 27.],\n","        [40., 55., 72.]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"VUAZ3_b0vOKv"},"source":["The difference between **element-wise** multiplication and **matrix** multiplication is the **addition of values**. For our `tensor` variable with values `[1, 2, 3]`:\n","\n","| Operation | Calculation | Code |\n","| ----- | ----- | ----- |\n","| **Element-wise multiplication** | `[1*1, 2*2, 3*3]` = `[1, 4, 9]` | `tensor * tensor` |\n","| **Matrix multiplication** | `[1*1 + 2*2 + 3*3]` = `[14]` | `tensor.matmul(tensor)` |\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i42gkUeHvI_1","outputId":"aeab4e5d-17c6-47a3-eb1f-7a79e7cc4c0f","executionInfo":{"status":"ok","timestamp":1686112213538,"user_tz":-540,"elapsed":460,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 4, 9])"]},"metadata":{},"execution_count":77}],"source":["# Element-wise matrix multiplication\n","tensor * tensor"]},{"cell_type":"markdown","metadata":{"id":"aJ4DDmo1TGe-"},"source":["## **3.5. Matrix Transpose**\n","\n","One of the most common errors in deep learning is **shape errors** (**shape mismatches**)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"rN5RcoD4Jo6y","outputId":"28497ccf-5938-47dc-a27c-72b89e0d9605","executionInfo":{"status":"error","timestamp":1686119812670,"user_tz":-540,"elapsed":462,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-93-aceec990e652>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                          [9, 12]], dtype=torch.float32)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"]}],"source":["# Shapes need to be in the right way\n","tensor_A = torch.tensor([[1, 2],\n","                         [3, 4],\n","                         [5, 6]], dtype=torch.float32)\n","\n","tensor_B = torch.tensor([[7, 10],\n","                         [8, 11],\n","                         [9, 12]], dtype=torch.float32)\n","\n","torch.matmul(tensor_A, tensor_B) # (this will error)"]},{"cell_type":"markdown","metadata":{"id":"HNA6MZEFxWVt"},"source":["We can make matrix multiplication work between `tensor_A` and `tensor_B` by making their inner dimensions match. One of the ways to do this is with a **transpose** (switch the dimensions of a given tensor). You can perform transposes in PyTorch using either:\n","* `torch.transpose(input, dim0, dim1)` - where `input` is the desired tensor to transpose and `dim0` and `dim1` are the dimensions to be swapped.\n","* `tensor.T` - where `tensor` is the desired tensor to transpose.\n","\n","Let's try the latter."]},{"cell_type":"code","source":["# View tensor_A and tensor_B\n","print(tensor_A)\n","print(tensor_B)"],"metadata":{"id":"pGajKEzzlTgM","executionInfo":{"status":"ok","timestamp":1686120012375,"user_tz":-540,"elapsed":257,"user":{"displayName":"","userId":""}},"outputId":"af2f74d9-e413-412e-b94f-e1fca7028118","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7., 10.],\n","        [ 8., 11.],\n","        [ 9., 12.]])\n"]}]},{"cell_type":"code","source":["# View tensor_A.T and tensor_B\n","print(tensor_A)\n","print(tensor_B.T)"],"metadata":{"id":"rTHsAh7flTo_","executionInfo":{"status":"ok","timestamp":1686120015185,"user_tz":-540,"elapsed":442,"user":{"displayName":"","userId":""}},"outputId":"b87bb177-4e57-4913-bbf0-a8d3ae724bae","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7.,  8.,  9.],\n","        [10., 11., 12.]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DveqxO7iy_Fi","outputId":"09f8b97a-9253-4c26-b9b0-dbc5729f7614","executionInfo":{"status":"ok","timestamp":1686119969240,"user_tz":-540,"elapsed":405,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7.,  8.,  9.],\n","        [10., 11., 12.]])\n"]}],"source":["# View tensor_A and tensor_B.T\n","print(tensor_A)\n","print(tensor_B.T)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35rEIu-NKtVE","outputId":"13ac74a9-dbcf-43f8-b81c-171d4770ac16","executionInfo":{"status":"ok","timestamp":1686123217358,"user_tz":-540,"elapsed":234,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n","\n","New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n","\n","Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n","\n","Output:\n","\n","tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])\n","\n","Output shape: torch.Size([3, 3])\n"]}],"source":["# The operation works when one of the two tensors is transposed\n","print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n","print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n","print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n","print(\"Output:\\n\")\n","output = torch.matmul(tensor_A, tensor_B.T)\n","print(output)\n","print(f\"\\nOutput shape: {output.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"MfcFEqfLjN24"},"source":["You can also use [`torch.mm()`](https://pytorch.org/docs/stable/generated/torch.mm.html) which is a short for `torch.matmul()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3rJvW_TTGe_","outputId":"29c0841b-8837-4912-be12-0c0a50e08082","executionInfo":{"status":"ok","timestamp":1686120142163,"user_tz":-540,"elapsed":454,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])"]},"metadata":{},"execution_count":99}],"source":["# torch.mm is a shortcut for matmul\n","torch.mm(tensor_A, tensor_B.T)"]},{"cell_type":"markdown","metadata":{"id":"bXKozI4T0hFi"},"source":["**Note:** A matrix multiplication like this is also referred to as the **dot product** of two matrices.\n","\n"]},{"cell_type":"markdown","source":["**Mismatched Data Types**"],"metadata":{"id":"EEkqk6v0ChOe"}},{"cell_type":"code","source":["a = torch.tensor([1, 2, 3])  # integer tensor\n","b = torch.tensor([1.0, 2.0, 3.0])  # float tensor\n","\n","c = torch.matmul(a, b)  # Error: Mismatched data types"],"metadata":{"id":"EsE7JRgMBEBw","executionInfo":{"status":"error","timestamp":1686110745337,"user_tz":-540,"elapsed":250,"user":{"displayName":"","userId":""}},"outputId":"4975d7c4-a4a3-45f5-9115-1bac3b03dc10","colab":{"base_uri":"https://localhost:8080/","height":236}},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-34555c4ecc04>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# float tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Error: Mismatched data types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Error message: RuntimeError: matmul(): Expected dtype Float but got dtype Long for argument 'other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: dot : expected both vectors to have same dtype, but found Long and Float"]}]},{"cell_type":"markdown","metadata":{"id":"hA64Z4DmkB31"},"source":["## **3.6. Linear Transformation in NN**\n","\n","**Neural networks** are full of matrix multiplications and dot products. The [`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) module (we'll see this in action later on), also known as a **feed-forward** layer or **fully connected** layer, implements a matrix multiplication between an input `x` and a weights matrix `W`.\n","\n","$$\n","{\\bf y}_{n\\times 1} = X_{n\\times J}W_{1\\times J}' + {\\bf b}_{n\\times 1}\n","$$\n","????????????\n","$$\n","y_i = {\\bf x}_i^{(1\\times J)} W_{1\\times J}' + b_i; i=1,2,...,n\n","$$\n","\n","Where:\n","* `x` is the input to the layer (deep learning is a stack of layers like `torch.nn.Linear()` and others on top of each other).\n","* `W` is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"`T`\", that's because the weights matrix gets transposed).\n","\n","* `b` is the bias term used to slightly offset the weights and inputs.\n","* `y` is the output (a manipulation of the input in the hopes to discover patterns in it).\n","\n","Let's play around with a linear layer. Try changing the values of `in_features` and `out_features` below and see what happens. Do you notice anything to do with the shapes?"]},{"cell_type":"code","source":["# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n","torch.manual_seed(42)\n","# This uses matrix multiplication\n","linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input\n","                         out_features=1) # out_features = describes outer value\n","x = torch.tensor([[1, 2],\n","                  [3, 4],\n","                  [5, 6]], dtype=torch.float32)\n","print(f\"Input shape: {x.shape}\\n\")\n","output = linear(x)\n","print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"],"metadata":{"id":"LlR95rKHtnCZ","executionInfo":{"status":"ok","timestamp":1686122181295,"user_tz":-540,"elapsed":247,"user":{"displayName":"","userId":""}},"outputId":"ac85ef02-a680-46fd-b952-93607628c1e5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([3, 2])\n","\n","Output:\n","tensor([[1.5488],\n","        [3.8038],\n","        [6.0588]], grad_fn=<AddmmBackward0>)\n","\n","Output shape: torch.Size([3, 1])\n"]}]},{"cell_type":"code","source":["# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n","torch.manual_seed(42)\n","# This uses matrix multiplication\n","linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input\n","                         out_features=2) # out_features = describes outer value\n","x = torch.tensor([[1, 2],\n","                  [3, 4],\n","                  [5, 6]], dtype=torch.float32)\n","print(f\"Input shape: {x.shape}\\n\")\n","output = linear(x)\n","print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"],"metadata":{"id":"PDxQvcI-twaP","executionInfo":{"status":"ok","timestamp":1686122211687,"user_tz":-540,"elapsed":306,"user":{"displayName":"","userId":""}},"outputId":"87fcdc33-dfe9-4685-9743-e49d3aa71062","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([3, 2])\n","\n","Output:\n","tensor([[1.5595, 1.2761],\n","        [3.8145, 2.2439],\n","        [6.0695, 3.2117]], grad_fn=<AddmmBackward0>)\n","\n","Output shape: torch.Size([3, 2])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mC_MjKW1LX7T","outputId":"0369025a-f828-4833-b7ee-4a38a21b78b2","executionInfo":{"status":"ok","timestamp":1686121152244,"user_tz":-540,"elapsed":254,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([3, 2])\n","\n","Output:\n","tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n","        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n","        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n","       grad_fn=<AddmmBackward0>)\n","\n","Output shape: torch.Size([3, 6])\n"]}],"source":["# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n","torch.manual_seed(42)\n","# This uses matrix multiplication\n","linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input\n","                         out_features=6) # out_features = describes outer value\n","x = torch.tensor([[1, 2],\n","                  [3, 4],\n","                  [5, 6]], dtype=torch.float32)\n","print(f\"Input shape: {x.shape}\\n\")\n","output = linear(x)\n","print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"]},{"cell_type":"markdown","source":["$$\n","y_{n\\times 1} = x_{n\\times k}\\cdot{W_{n\\times k}^T} + b_{n\\times 1}?\n","$$"],"metadata":{"id":"t3mKfan6q8uo"}},{"cell_type":"markdown","metadata":{"id":"zIGrP5j1pN7j"},"source":["> **Question:** What happens if you change `in_features` from 2 to 3 above? Does it error? How could you change the shape of the input (`x`) to accomodate to the error? Hint: what did we have to do to `tensor_B` above?"]},{"cell_type":"markdown","metadata":{"id":"pjMmrJOOPv5e"},"source":["## **3.7. Tensor Aggregation**\n","\n","Find the **max**, **min**, **mean** and **sum** of it. Note that you may find some methods such as `tensor.mean()` require tensors to be in `torch.float32` (the most common) or another specific datatype, otherwise the operation will fail.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrFQbe5fP1Rk","outputId":"cb4f13f4-4624-44a0-ea07-5315f2fb7b60","executionInfo":{"status":"ok","timestamp":1686127371280,"user_tz":-540,"elapsed":465,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"]},"metadata":{},"execution_count":6}],"source":["# Create a tensor\n","x = torch.arange(0, 100, 10)\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5wSP9YKP3Lb","outputId":"314bd614-4d8d-4e0c-cdae-824b3462a845","executionInfo":{"status":"ok","timestamp":1686127373354,"user_tz":-540,"elapsed":2,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum: 0\n","Maximum: 90\n","Mean: 45.0\n","Sum: 450\n"]}],"source":["print(f\"Minimum: {x.min()}\")\n","print(f\"Maximum: {x.max()}\")\n","# print(f\"Mean: {x.mean()}\") # this will error\n","print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n","print(f\"Sum: {x.sum()}\")"]},{"cell_type":"markdown","metadata":{"id":"JHoKpsg3sKQE"},"source":["\n","\n","You can also do the same as above with `torch` methods."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Cr23Y9uP3HO","outputId":"f3410642-c846-47e1-af20-29c0999e0984","executionInfo":{"status":"ok","timestamp":1686127573945,"user_tz":-540,"elapsed":377,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(90), tensor(0), tensor(45.), tensor(450))"]},"metadata":{},"execution_count":10}],"source":["torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"]},{"cell_type":"markdown","metadata":{"id":"i7ApCaZjDkvp"},"source":["## **3.8. Positional Min/Max**\n","\n","You can also find the location index of a tensor **where the max or minimum occurs** with [`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) and [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html) respectively.\n","This is helpful in case we just want the **position where the highest (or lowest) value is** and **not the actual value itself** (we'll see this in a later section when using the [softmax activation function](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzNBl9JSGlHi","outputId":"fb8d4dd5-881f-4440-c572-d1368326d92d","executionInfo":{"status":"ok","timestamp":1686130829772,"user_tz":-540,"elapsed":372,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n","Index where max value occurs: 8\n","Index where min value occurs: 0\n"]}],"source":["# Create a tensor\n","tensor = torch.arange(10, 100, 10)\n","print(f\"Tensor: {tensor}\")\n","\n","# Returns index of max and min values\n","print(f\"Index where max value occurs: {tensor.argmax()}\")\n","print(f\"Index where min value occurs: {tensor.argmin()}\")"]},{"cell_type":"code","source":["tensor[0], tensor[8]"],"metadata":{"id":"p7Z-owVXNhwW","executionInfo":{"status":"ok","timestamp":1686130845670,"user_tz":-540,"elapsed":356,"user":{"displayName":"","userId":""}},"outputId":"3a3d93e9-609c-4818-e83e-ecbc2e315cc6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(10), tensor(90))"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"7CkCtAYmGsHY"},"source":["## **3.9. Reshaping, stacking, squeezing and unsqueezing**\n","\n","Often times you'll want to reshape or change the dimensions of your tensors without actually changing the values inside them. To do so, some popular methods are:\n","\n","> **Reshaping**: Reshaping refers to **changing the shape of a tensor** while maintaining the same number of elements. In PyTorch, the common operation to reshape a tensor is `view()`, which allows you to specify the desired shape. For example, you can reshape a tensor of shape (2, 3) to (3, 2) using `tensor.view(3, 2)`.\n","\n","> **Stacking**: Stacking involves **combining multiple tensors** along a new dimension. PyTorch provides the `torch.stack()` function, which stacks tensors along a specified dimension. For example, if you have two tensors of shape (2, 3) and (2, 3), you can stack them along the 0th dimension using `torch.stack([tensor1, tensor2], dim=0)` to get a resulting tensor of shape (2, 2, 3). There are also addtional functions to create mutliple tensors on top of each other `vstack()` or side by side `hstack()`.\n","\n","> **Squeezing**: Squeezing is the operation of **removing dimensions with size 1** from a tensor. It reduces the rank of the tensor. The `torch.squeeze()` function is used to remove dimensions of size 1. For example, if you have a tensor of shape (1, 3, 1, 5), you can remove the dimensions of size 1 using `torch.squeeze(tensor)` to get a resulting tensor of shape (3, 5).\n","\n","> **Unsqueezing**: Unsqueezing is the opposite operation of squeezing. It **adds dimensions with size 1** to a tensor. The `torch.unsqueeze()` function is used to add dimensions of size 1 at a specified position. For example, if you have a tensor of shape (3, 5), you can unsqueeze it along dimension 0 using `torch.unsqueeze(tensor, dim=0)` to get a resulting tensor of shape (1, 3, 5).\n","\n","| Method | One-line description |\n","| ----- | ----- |\n","| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | Reshapes `input` to `shape` (if compatible), can also use `torch.Tensor.reshape()`. |\n","| [`torch.Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) | Returns a view of the original tensor in a different `shape` but shares the same data as the original tensor. |\n","| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html) | Concatenates a sequence of `tensors` along a new dimension (`dim`), all `tensors` must be same size. |\n","| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) | Squeezes `input` to remove all the dimenions with value `1`. |\n","| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html) | Returns `input` with a dimension value of `1` added at `dim`. |\n","| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html) | Returns a *view* of the original `input` with its dimensions permuted (rearranged) to `dims`. |\n","\n","Why do any of these?\n","\n","Because deep learning models (neural networks) are all about **manipulating tensors** in some way. And because of the rules of matrix multiplication, if you've got shape mismatches, you'll run into errors. These methods help you make the right elements of your tensors are mixing with the right elements of other tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYjRTLOzG4Ev","outputId":"6b107946-da71-44f5-d47e-3e49b8f2e074","executionInfo":{"status":"ok","timestamp":1686270986285,"user_tz":-540,"elapsed":5791,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))"]},"metadata":{},"execution_count":1}],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1., 10.)\n","x, x.shape"]},{"cell_type":"markdown","metadata":{"id":"3_VarMO9CoT8"},"source":["Now let's add an extra dimension with `torch.reshape()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"US4WjpQ3SG-8","outputId":"8a8aad9d-6225-4a7c-f83d-72c5bc8e8f5d","executionInfo":{"status":"ok","timestamp":1686270986286,"user_tz":-540,"elapsed":12,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"]},"metadata":{},"execution_count":2}],"source":["# Add an extra dimension\n","x_reshaped = x.reshape(1, 9)\n","x_reshaped, x_reshaped.shape"]},{"cell_type":"code","source":["x_reshaped = x.reshape(9,1) # 9x1 = shape of tensor 9\n","x_reshaped, x_reshaped.shape"],"metadata":{"id":"BmHIOF03oK2Z","executionInfo":{"status":"ok","timestamp":1686270986286,"user_tz":-540,"elapsed":7,"user":{"displayName":"","userId":""}},"outputId":"b671bc77-7580-4722-dd79-9ac427848b60","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1.],\n","         [2.],\n","         [3.],\n","         [4.],\n","         [5.],\n","         [6.],\n","         [7.],\n","         [8.],\n","         [9.]]),\n"," torch.Size([9, 1]))"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x_reshaped = x.reshape(3,3) # 3x3 = shape of tensor 9\n","x_reshaped, x_reshaped.shape"],"metadata":{"id":"NPdtgF0EpepU","executionInfo":{"status":"ok","timestamp":1686270987056,"user_tz":-540,"elapsed":2,"user":{"displayName":"","userId":""}},"outputId":"52012809-7313-4cc6-f8f1-45d8f01f74e6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3.],\n","         [4., 5., 6.],\n","         [7., 8., 9.]]),\n"," torch.Size([3, 3]))"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"tig5xm0jCxuU"},"source":["We can also change the view with `torch.view()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDN2BNe5TGfB","outputId":"3a7fdce0-e3fd-495b-b4ac-f7c280fca85e","executionInfo":{"status":"ok","timestamp":1686222628928,"user_tz":-540,"elapsed":245,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"]},"metadata":{},"execution_count":33}],"source":["# Change view (keeps same data as original but changes view)\n","z = x.view(1, 9)\n","z, z.shape"]},{"cell_type":"markdown","metadata":{"id":"m8joAaUEC2NX"},"source":["Remember though, changing the view of a tensor with `torch.view()` really only creates a new view of the *same* tensor. So changing the view changes the original tensor too because it shares the same memory as the original one."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DxURVvXTGfC","outputId":"6324076b-e0c4-4c13-9a86-f4bf6d69725d","executionInfo":{"status":"ok","timestamp":1686222632104,"user_tz":-540,"elapsed":248,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n"," tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.]))"]},"metadata":{},"execution_count":34}],"source":["# Changing z changes x\n","z[:, 0] = 5 # first element is changed to 5\n","z, x"]},{"cell_type":"code","source":["# Reshape with torch.view()\n","x = torch.randn(4, 4)\n","print(x)\n","y = x.view(16) # 1D tensor, 16=4*4, no of items should be equal\n","print(y)\n","z = x.view(-1, 8)  # a by 8 tensor, -1 makes a to be determined by default\n","print(z)\n","print(x.size(), y.size(), z.size())"],"metadata":{"id":"3tw50H7kB2RA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YxnqDBlpDDJ_"},"source":["If we wanted to stack our new tensor on top of itself five times, we could do so with `torch.stack()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pX5Adf3ORiTK","outputId":"c0db5968-3519-42af-c4fe-d2d3fe640c7c","executionInfo":{"status":"ok","timestamp":1686222634986,"user_tz":-540,"elapsed":259,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],\n","        [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n","        [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n","        [5., 2., 3., 4., 5., 6., 7., 8., 9.]])"]},"metadata":{},"execution_count":35}],"source":["# Stack tensors on top of each other\n","x_stacked = torch.stack([x, x, x, x], dim=0) # stacked vertically\n","x_stacked"]},{"cell_type":"code","source":["# Stack tensors on top of each other\n","x_stacked = torch.stack([x, x, x, x], dim=1) # stacked horizontally\n","x_stacked"],"metadata":{"id":"JDWJDryxqtJL","executionInfo":{"status":"ok","timestamp":1686222638136,"user_tz":-540,"elapsed":309,"user":{"displayName":"","userId":""}},"outputId":"44ccfa70-a5d3-436e-a599-a2d64c238e69","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[5., 5., 5., 5.],\n","        [2., 2., 2., 2.],\n","        [3., 3., 3., 3.],\n","        [4., 4., 4., 4.],\n","        [5., 5., 5., 5.],\n","        [6., 6., 6., 6.],\n","        [7., 7., 7., 7.],\n","        [8., 8., 8., 8.],\n","        [9., 9., 9., 9.]])"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"ET56QzNHDuOI"},"source":["How about removing all single dimensions from a tensor?\n","To do so you can use `torch.squeeze()` (I remember this as **squeezing** the tensor to only have dimensions over 1)."]},{"cell_type":"code","source":["y = torch.arange(1., 10.)\n","y_reshaped = y.reshape(1, 9)\n","print(f\"Previous tensor: {y_reshaped}\")\n","print(f\"Previous shape: {y_reshaped.shape}\")\n","\n","# Remove extra dimension from x_reshaped\n","y_squeezed = y_reshaped.squeeze() # one square bracket is removed\n","print(f\"\\nNew tensor: {y_squeezed}\")\n","print(f\"New shape: {y_squeezed.shape}\")"],"metadata":{"id":"lbbwva-Os8Fl","executionInfo":{"status":"ok","timestamp":1686222871218,"user_tz":-540,"elapsed":265,"user":{"displayName":"","userId":""}},"outputId":"eb3c72d8-3964-46a3-9220-f65067252b45","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n","Previous shape: torch.Size([1, 9])\n","\n","New tensor: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n","New shape: torch.Size([9])\n"]}]},{"cell_type":"markdown","metadata":{"id":"acjDLk8WD8NC"},"source":["And to do the reverse of `torch.squeeze()`, you can use `torch.unsqueeze()` to add a dimension value of 1 at a specific index."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUC-DEEwSYv7","outputId":"a1b78c16-f9cd-40b8-e2fa-bc6411846dda","executionInfo":{"status":"ok","timestamp":1686223497778,"user_tz":-540,"elapsed":259,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n","Previous shape: torch.Size([9])\n","\n","New tensor: tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n","New shape: torch.Size([1, 9])\n"]}],"source":["print(f\"Previous tensor: {y_squeezed}\")\n","print(f\"Previous shape: {y_squeezed.shape}\")\n","\n","## Add an extra dimension with unsqueeze\n","y_unsqueezed = y_squeezed.unsqueeze(dim=0)\n","print(f\"\\nNew tensor: {y_unsqueezed}\")\n","print(f\"New shape: {y_unsqueezed.shape}\")"]},{"cell_type":"code","source":["print(f\"Previous tensor: {y_squeezed}\")\n","print(f\"Previous shape: {y_squeezed.shape}\")\n","\n","## Add an extra dimension with unsqueeze\n","y_unsqueezed = y_squeezed.unsqueeze(dim=1) # dim = 1\n","print(f\"\\nNew tensor: {y_unsqueezed}\")\n","print(f\"New shape: {y_unsqueezed.shape}\")"],"metadata":{"id":"GfA0YndRwyL2","executionInfo":{"status":"ok","timestamp":1686223677337,"user_tz":-540,"elapsed":251,"user":{"displayName":"","userId":""}},"outputId":"41bae43f-f440-42d6-b12a-6a5f8d242e86","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n","Previous shape: torch.Size([9])\n","\n","New tensor: tensor([[1.],\n","        [2.],\n","        [3.],\n","        [4.],\n","        [5.],\n","        [6.],\n","        [7.],\n","        [8.],\n","        [9.]])\n","New shape: torch.Size([9, 1])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2Y2HEoDRxJZ","outputId":"11c62f31-888c-423d-f353-ab164bc1f426","executionInfo":{"status":"ok","timestamp":1686271100138,"user_tz":-540,"elapsed":283,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","Previous shape: torch.Size([3, 3])\n","\n","New tensor: tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","New shape: torch.Size([3, 3])\n"]}],"source":["print(f\"Previous tensor: {x_reshaped}\")\n","print(f\"Previous shape: {x_reshaped.shape}\")\n","\n","# Remove extra dimension from x_reshaped\n","x_squeezed = x_reshaped.squeeze()\n","print(f\"\\nNew tensor: {x_squeezed}\")\n","print(f\"New shape: {x_squeezed.shape}\")"]},{"cell_type":"markdown","source":["The `unsqueeze` method in PyTorch does **not have a default dimension**. It requires you to explicitly specify the dimension along which you want to add a singleton dimension (dimension of size 1)."],"metadata":{"id":"0lCwT5kRmsu4"}},{"cell_type":"code","source":["print(f\"Previous tensor: {x_reshaped}\")\n","print(f\"Previous shape: {x_reshaped.shape}\")\n","\n","# Remove extra dimension from x_reshaped\n","x_unsqueezed = x_reshaped.unsqueeze(dim=0) # dim = 0\n","print(f\"\\nNew tensor: {x_unsqueezed}\")\n","print(f\"New shape: {x_unsqueezed.shape}\")"],"metadata":{"id":"fx6wk158lPWN","executionInfo":{"status":"ok","timestamp":1686271134702,"user_tz":-540,"elapsed":384,"user":{"displayName":"","userId":""}},"outputId":"674ab982-6a31-4774-a884-2f638434b020","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","Previous shape: torch.Size([3, 3])\n","\n","New tensor: tensor([[[1., 2., 3.],\n","         [4., 5., 6.],\n","         [7., 8., 9.]]])\n","New shape: torch.Size([1, 3, 3])\n"]}]},{"cell_type":"code","source":["print(f\"Previous tensor: {x_reshaped}\")\n","print(f\"Previous shape: {x_reshaped.shape}\")\n","\n","# Remove extra dimension from x_reshaped\n","x_unsqueezed = x_reshaped.unsqueeze(dim=1) # dim = 1\n","print(f\"\\nNew tensor: {x_unsqueezed}\")\n","print(f\"New shape: {x_unsqueezed.shape}\")"],"metadata":{"id":"89lQA8EtmBix","executionInfo":{"status":"ok","timestamp":1686271184418,"user_tz":-540,"elapsed":259,"user":{"displayName":"","userId":""}},"outputId":"685938dd-c8ab-4ab7-fe32-065918ce89fd","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","Previous shape: torch.Size([3, 3])\n","\n","New tensor: tensor([[[1., 2., 3.]],\n","\n","        [[4., 5., 6.]],\n","\n","        [[7., 8., 9.]]])\n","New shape: torch.Size([3, 1, 3])\n"]}]},{"cell_type":"markdown","metadata":{"id":"R9DuJzXgFbM5"},"source":["You can also rearrange the order of axes values with `torch.permute(input, dims)`, where the `input` gets turned into a *view* with new `dims`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCRGCX8DTGfC","outputId":"581c89de-a0f6-4a95-e585-80592d52a94d","executionInfo":{"status":"ok","timestamp":1686271013471,"user_tz":-540,"elapsed":243,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous shape: torch.Size([224, 224, 3])\n","New shape: torch.Size([3, 224, 224])\n"]}],"source":["# Create tensor with specific shape\n","x_original = torch.rand(size=(224, 224, 3)) # [height, width, color_channels]\n","\n","# Permute the original tensor to rearrange the axis (or dim) order\n","x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n","                                         # [color_channels, height, width]\n","\n","print(f\"Previous shape: {x_original.shape}\")\n","print(f\"New shape: {x_permuted.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"06LKaFemGBoE"},"source":["> **Note**: Because permuting returns a **view** (**shares the same data as the original**), the values in the permuted tensor will be the same as the original tensor and if you change the values in the view, it will change the values of the original."]},{"cell_type":"markdown","source":["# **4. Gradient Computation**\n","\n","In PyTorch, there is an attribute of tensors called `requires_grad` property that allows for **automatic differentiation** and **gradient computation** during backpropagation. It is used to determine whether gradients need to be computed for a particular tensor as part of the **computational graph**.\n","When you create a tensor in PyTorch, by default, the `requires_grad` property is set to `False`. This means that the tensor does **not track operations** on it and does **not participate in gradient computation**. However, if you want to compute gradients with respect to that tensor, you can set `requires_grad` to `True`.\n","\n","Below are examples to illustrate the concept:"],"metadata":{"id":"Wje_eQvVaxKz"}},{"cell_type":"code","source":["x = torch.rand(3, requires_grad=True)\n","print(x)\n","y = x + 2\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQInG80oEFXE","executionInfo":{"status":"ok","timestamp":1686532142931,"user_tz":-540,"elapsed":268,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"ccf0aa69-403e-4f31-a47d-be09858821d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9979, 0.0458, 0.9733], requires_grad=True)\n","tensor([2.9979, 2.0458, 2.9733], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","source":["The `grad_fn` attribute references the function that created the tensor and keeps track of the operations applied to the tensor.\n","In the example, `grad_fn=<AddBackward0>` indicates that the tensor was created by **adding two or more tensors** together using the `torch.add()` function or the `+` operator. The `AddBackward0` represents the specific function used for the addition operation, and the `0` refers to the fact that it is the first occurrence of the backward operation for this particular addition."],"metadata":{"id":"UMrm8bofFr4u"}},{"cell_type":"code","source":["x = torch.rand(3, requires_grad=True)\n","print(x)\n","print(x.requires_grad)\n","print(x.grad_fn)\n","y = x - 2\n","print(y)\n","print(y.requires_grad)\n","print(y.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQ42yJZcFWL1","executionInfo":{"status":"ok","timestamp":1686534046868,"user_tz":-540,"elapsed":437,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"0d5e6e76-b91e-4c80-8740-afbb69ff4f18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.7659, 0.8687, 0.4430], requires_grad=True)\n","True\n","None\n","tensor([-1.2341, -1.1313, -1.5570], grad_fn=<SubBackward0>)\n","True\n","<SubBackward0 object at 0x7fba8eaf99f0>\n"]}]},{"cell_type":"code","source":["x = torch.rand(3, requires_grad=True)\n","print(x)\n","y = x * 2\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSf-iT1wFaIE","executionInfo":{"status":"ok","timestamp":1686531064120,"user_tz":-540,"elapsed":295,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"e2991fd4-16dc-48b4-9941-6342a3133bcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.1603, 0.3280, 0.1625], requires_grad=True)\n","tensor([0.3205, 0.6561, 0.3250], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","source":["x = torch.rand(3, requires_grad=True)\n","print(x)\n","y = x / 2\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6cEms6OFc_Z","executionInfo":{"status":"ok","timestamp":1686531074790,"user_tz":-540,"elapsed":323,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"d555e07d-8dbf-4386-ca28-396d2a3d6524"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9482, 0.7802, 0.2670], requires_grad=True)\n","tensor([0.4741, 0.3901, 0.1335], grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["x = torch.rand(3, requires_grad=True)\n","print(x)\n","y = x**2\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjNVzIK4Ffi6","executionInfo":{"status":"ok","timestamp":1693737861050,"user_tz":-540,"elapsed":317,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"baefcd48-2e7b-4558-8155-81d659790a1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5760, 0.9867, 0.5467], requires_grad=True)\n","tensor([0.3318, 0.9735, 0.2989], grad_fn=<PowBackward0>)\n"]}]},{"cell_type":"code","source":["z = y*y*2\n","print(z)\n","z= z.mean()\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wSSBD-cINiS","executionInfo":{"status":"ok","timestamp":1693737875923,"user_tz":-540,"elapsed":359,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"2af62411-91b1-42d9-b0d2-1bcf161162e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.2201, 1.8954, 0.1786], grad_fn=<MulBackward0>)\n","tensor(0.7647, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","source":["Let's compute the gradients with **backpropagation**. When we finish our computation we can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute. It is the **partial derivate** of the function w.r.t. the tensor."],"metadata":{"id":"KSDA9fQHIk5d"}},{"cell_type":"markdown","source":["$$x\\rightarrow y=f(x)=x^2\\rightarrow z=g(y)=g(f(x))=2y^2$$\n","?????????????\n","\n","$$\n","\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial f}\\frac{\\partial f}{\\partial x}=4y (2x) =8x^3\n","$$\n"],"metadata":{"id":"r1qREyylgvkA"}},{"cell_type":"code","source":["z.backward() # Since z is a scalar, no need to put an argument in ()\n","print(x.grad) # dz/dx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZRxJqSxIsOW","executionInfo":{"status":"ok","timestamp":1693737891937,"user_tz":-540,"elapsed":347,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"e0c164c1-5e16-412e-a16d-17be74bfcb5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5096, 2.5614, 0.4357])\n"]}]},{"cell_type":"markdown","source":["Generally speaking, `torch.autograd` is an engine for computing **vector-Jacobian product**. It computes **partial derivates applying the chain rule**."],"metadata":{"id":"w2iRKW5UGtnB"}},{"cell_type":"code","source":["import torch\n","# Create a scalar tensor with requires_grad=True\n","x = torch.tensor(3.0, requires_grad=True)\n","\n","# Perform computations\n","y = 2 * x + 1\n","\n","# Calculate gradients\n","y.backward()\n","\n","# Access the gradient\n","print(x.grad)  # prints: tensor(2.)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kxqq4E25QYvI","executionInfo":{"status":"ok","timestamp":1686382953690,"user_tz":-540,"elapsed":236,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"0fe7f67a-7fc0-491c-bffb-becd0aabd6fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.)\n"]}]},{"cell_type":"code","source":["# Create a scalar tensor with requires_grad=True\n","x = torch.tensor(3.0, requires_grad=True)\n","\n","# Perform computations\n","y = 2 * x + 1 + x**2 + 1\n","\n","# Calculate gradients\n","y.backward()\n","\n","# Access the gradient\n","print(x.grad)  # prints: tensor(8.)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWaxQzO2Qofa","executionInfo":{"status":"ok","timestamp":1686383057183,"user_tz":-540,"elapsed":247,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"164c3f2c-9614-46ef-8bf8-bb593c1b9a4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(8.)\n"]}]},{"cell_type":"code","source":["# Create a tensor without gradient tracking\n","x = torch.tensor([1, 2, 3])\n","\n","# Set requires_grad=True to enable gradient tracking\n","x = torch.tensor([1, 2, 3], requires_grad=True)\n","\n","# Perform some computations\n","y = x * 2\n","z = y.mean()\n","\n","# Perform backpropagation\n","z.backward()\n","\n","# Access gradients\n","print(x.grad)"],"metadata":{"id":"o1bub6rOctWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a tensor and enable gradient tracking\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","\n","# Perform some computations\n","y = x.pow(3).sum()\n","y"],"metadata":{"id":"KEUnbVd98sRW","executionInfo":{"status":"ok","timestamp":1686383337834,"user_tz":-540,"elapsed":261,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"74407651-5b32-426a-9dff-15d5cf482618","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(36., grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Calculate gradients\n","y.backward()\n","y"],"metadata":{"id":"NQdtd2F19G47","executionInfo":{"status":"ok","timestamp":1686383340237,"user_tz":-540,"elapsed":317,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"71720385-9fdf-4857-d85c-0b8799050d9b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(36., grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Access the gradients\n","print(x.grad)  # prints: tensor([3., 12., 27.])"],"metadata":{"id":"QLpdY7CG9QUf","executionInfo":{"status":"ok","timestamp":1686383342786,"user_tz":-540,"elapsed":260,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"ced4c14b-fa6c-48da-fbc4-bf746f19ef09","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 3., 12., 27.])\n"]}]},{"cell_type":"code","source":["# Create a tensor and enable gradient tracking\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","\n","# Perform some computations\n","y = x.pow(2).sum()\n","\n","# Calculate gradients\n","y.backward()\n","\n","# Access the gradients\n","print(x.grad)  # prints: tensor([2., 4., 6.])"],"metadata":{"id":"gIyB5tjf-u2B","executionInfo":{"status":"ok","timestamp":1686109882888,"user_tz":-540,"elapsed":253,"user":{"displayName":"","userId":""}},"outputId":"e9689bac-9d42-4b40-b680-d29b9397a549","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 4., 6.])\n"]}]},{"cell_type":"markdown","source":["If a tensor is non-scalar (more than 1 elements), we need to specify arguments for `backward()` specify a gradient argument that is a tensor of matching shape. Needed for vector-Jacobian product."],"metadata":{"id":"l9o0SwsoKyMR"}},{"cell_type":"code","source":["x = torch.randn(3, requires_grad=True)\n","print(x)\n","y = x + 2\n","print(y)\n","z = y*y*2\n","print(z)\n","v= torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n","print(v)\n","z.backward(v)\n","print(x.grad) # dz/dx"],"metadata":{"id":"7DJfi9ehKy-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","for _ in range(10):\n","    y = y * 2\n","print(y)\n","print(y.shape)\n","v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n","y.backward(v)\n","print(x.grad)"],"metadata":{"id":"zkBsMp42K2GT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Gradient tracking compatibility**: Note that **not all operations support gradient computation**, especially those that involve **non-differentiable operations like indexing or integer division**."],"metadata":{"id":"wSpvzo28fR_T"}},{"cell_type":"markdown","source":["## **4.1. Disabling Gradient Tracking**\n","\n","PyTorch does **not support in-place operations** on tensors that are part of the computational graph for **gradient calculation** because it can lead to **incorrect or inconsistent gradient calculations**. If you specifically want to perform in-place operations on a tensor that requires gradients, you can detach the tensor before the operation. **Detaching a tensor removes it from the computational graph and disables gradient tracking.** However, keep in mind that detached tensors will not have gradients, so you won't be able to backpropagate through them.\n","\n","Three options: 1) `x.requires_grad_(False)`; 2) `x.detach()`; 3) wrap in context manager '`with torch.no_grad():` or `torch.inference_mode()`'.\n","\n","By detaching a tensor, you can effectively prevent it from participating in subsequent gradient computations, **reducing memory consumption** and **computation overhead** during backpropagation."],"metadata":{"id":"FZJU6dHIMLF6"}},{"cell_type":"code","source":["x = torch.randn(3, requires_grad=True)\n","print(x)\n","x = x.requires_grad_(False)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJsIDArhhuWj","executionInfo":{"status":"ok","timestamp":1686533574715,"user_tz":-540,"elapsed":253,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"53d55ebd-cefd-486e-873c-6a0e4002d87e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.0670, -1.6403,  0.1405], requires_grad=True)\n","tensor([ 1.0670, -1.6403,  0.1405])\n"]}]},{"cell_type":"code","source":["a = torch.randn(2, 2)\n","print(a)\n","print(a.requires_grad)\n","\n","b = ((a * 3) / (a - 1))\n","print(b)\n","print(b.grad_fn)\n","\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","\n","b = (a * a).sum()\n","print(b)\n","print(b.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BZf7NVQTrzs","executionInfo":{"status":"ok","timestamp":1686533929492,"user_tz":-540,"elapsed":342,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"f92bf098-ca6e-4ab0-e44f-14d4667c456d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.7842,  0.2817],\n","        [-1.9815, -0.6158]])\n","False\n","tensor([[ 6.8256, -1.1764],\n","        [ 1.9938,  1.1433]])\n","None\n","True\n","tensor(7.5680, grad_fn=<SumBackward0>)\n","<SumBackward0 object at 0x7fba8eaf8af0>\n"]}]},{"cell_type":"markdown","source":["`x.detach()` get a new Tensor with the same content but no gradient computation:"],"metadata":{"id":"0Yzrzcw0iL17"}},{"cell_type":"code","source":["a = torch.randn(2, 2, requires_grad=True)\n","print(a)\n","print(a.requires_grad)\n","b = a.detach()\n","print(b)\n","print(b.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBoy4LqeTr2u","executionInfo":{"status":"ok","timestamp":1653705931564,"user_tz":-540,"elapsed":266,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"0a05b208-a956-455b-cb16-2a3915e2fe49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.2120, -0.1929],\n","        [-0.0765, -1.0866]], requires_grad=True)\n","True\n","tensor([[ 0.2120, -0.1929],\n","        [-0.0765, -1.0866]])\n","False\n"]}]},{"cell_type":"markdown","source":["Wrap in '`with torch.no_grad():`/`torch.inference_mode():`"],"metadata":{"id":"g1FLxUpnirJt"}},{"cell_type":"code","source":["a = torch.randn(2, 2, requires_grad=True)\n","print(a)\n","print(a.requires_grad)\n","with torch.no_grad():\n","  y = a**2\n","  print(y)\n","  print(y.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXhwXrCGTr5c","executionInfo":{"status":"ok","timestamp":1686534107770,"user_tz":-540,"elapsed":266,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"456f0f0e-be01-4fcd-fb7f-7aedb7fac3b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.4048, -0.5917],\n","        [ 0.3134, -0.4440]], requires_grad=True)\n","True\n","tensor([[0.1638, 0.3501],\n","        [0.0982, 0.1971]])\n","False\n"]}]},{"cell_type":"code","source":["a = torch.randn(2, 2, requires_grad=True)\n","print(a)\n","print(a.requires_grad)\n","with torch.inference_mode():\n","  y = a**2\n","  print(y)\n","  print(y.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3M2YRjTERG-o","executionInfo":{"status":"ok","timestamp":1686534141393,"user_tz":-540,"elapsed":262,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"34acb4f7-93b9-4843-c078-1188685085dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.4532, 2.4012],\n","        [0.9637, 1.3640]], requires_grad=True)\n","True\n","tensor([[6.0181, 5.7656],\n","        [0.9287, 1.8604]])\n","False\n"]}]},{"cell_type":"markdown","source":["## **4.2. Setting Gradients to Zero**\n","\n","`backward()` accumulates the gradient for this tensor into `.grad` attribute. We need to be careful during optimization !!! Use `.zero_()` to empty the gradients before a new optimization step!\n","\n","Suppose $y$ is a function to be minimized interms of $w$."],"metadata":{"id":"xyMNIBwLVZgx"}},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(1):\n","    # just a dummy operation\n","    y = (w*x).sum()   # forward pass\n","    y.backward()      # backward pass: dy/dw\n","    print(w.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTSGa4BUln4s","executionInfo":{"status":"ok","timestamp":1686535622122,"user_tz":-540,"elapsed":2,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"817f75cc-c92a-44c5-ac85-44c759a2004e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n"]}]},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(2):\n","    y = (w*x).sum()\n","    y.backward()\n","    print(w.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLLf7uxml9sQ","executionInfo":{"status":"ok","timestamp":1686535648085,"user_tz":-540,"elapsed":293,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"dce71359-4b59-46ee-c629-58eac647ddf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([6., 6., 6., 6.])\n"]}]},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(5):\n","    y = (w*x).sum()\n","    y.backward()\n","    print(w.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiy53Yc4mIdi","executionInfo":{"status":"ok","timestamp":1686535675110,"user_tz":-540,"elapsed":301,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"89852ae2-df75-4549-c323-e95956d6715a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([6., 6., 6., 6.])\n","tensor([9., 9., 9., 9.])\n","tensor([12., 12., 12., 12.])\n","tensor([15., 15., 15., 15.])\n"]}]},{"cell_type":"markdown","source":["Hence, at each step of iteration, gradients are accumulated."],"metadata":{"id":"m1akWhHJmX8w"}},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(5):\n","    y = (w*x).sum()\n","    y.backward()\n","    print(w.grad)\n","    # this is important!\n","    w.grad.zero_()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6jFv7FImWU4","executionInfo":{"status":"ok","timestamp":1686535718000,"user_tz":-540,"elapsed":371,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"27b329b3-9621-4f99-a8fe-ac7d26cfbb6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n"]}]},{"cell_type":"markdown","source":["The gradients above are correct.\n","\n","Now, let us add an update part to the above dummy operation."],"metadata":{"id":"d2XUl8RqmnsT"}},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(5):\n","    y = (w*x).sum()\n","    y.backward()\n","    print(w.grad)\n","\n","    # optimize model, i.e. adjust weights...\n","    w -= 0.1 * w.grad\n","\n","print(w)\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"xxEZ4tKuYG8P","executionInfo":{"status":"error","timestamp":1686535974188,"user_tz":-540,"elapsed":398,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"8638924b-0cdd-4fa5-999f-ac931b34efd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-3e3cd51106e4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# optimize model, i.e. adjust weights...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."]}]},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(5):\n","    y = (w*x).sum()\n","    y.backward()\n","    print(w.grad)\n","\n","    # optimize model, i.e. adjust weights...\n","    with torch.no_grad():\n","        w -= 0.1 * w.grad\n","\n","print(w)\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvRG4hXLXo6_","executionInfo":{"status":"ok","timestamp":1686535891164,"user_tz":-540,"elapsed":283,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"72f4b537-face-4136-8d5a-3bd2a8c43b89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([6., 6., 6., 6.])\n","tensor([9., 9., 9., 9.])\n","tensor([12., 12., 12., 12.])\n","tensor([15., 15., 15., 15.])\n","tensor([-3.5000, -3.5000, -3.5000, -3.5000], requires_grad=True)\n","tensor(-24., grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"code","source":["x = 3\n","w = torch.ones(4, requires_grad=True)\n","\n","for step in range(5):\n","    y = (w*x).sum()\n","    y.backward()\n","    print(w.grad)\n","\n","    # optimize model, i.e. adjust weights...\n","    with torch.no_grad():\n","        w -= 0.1 * w.grad\n","\n","    # this is important! It affects the final weights w & output y\n","    w.grad.zero_()\n","\n","print(w)\n","print(y)"],"metadata":{"id":"5dDiYjdzNtyG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686535923303,"user_tz":-540,"elapsed":377,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"2949c072-aae1-46ab-9b82-1af8deb6688a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([-0.5000, -0.5000, -0.5000, -0.5000], requires_grad=True)\n","tensor(-2.4000, grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"markdown","source":["**Builtin Optimizers**: The `.zero_grad()` method is commonly used in deep learning to clear gradients for all the model's parameters before the backward pass and parameter update.\n","\n","The `.zero_grad()` method is different `grad.zero_()`. The `grad.zero_()` is used to zero out the gradients of a specific tensor. It is called on the grad attribute of a tensor and resets the gradients to zero for that **particular tensor**. It is typically used when you want to manually manage gradients or update **specific gradients independently**.\n","\n","Whereas the `.zero_grad()` method is used to zero out the gradients of **all parameters** that are being optimized by an optimizer. It is **called on an optimizer object and resets the gradients to zero for all the parameters that the optimizer is tracking**. It is a convenient way to zero the gradients of all the model's parameters at once."],"metadata":{"id":"x_Fm4boTm6Mn"}},{"cell_type":"code","source":["w = {\"w_0\": torch.tensor(1.0, requires_grad=True),\n","     \"w_1\": torch.tensor(1.0, requires_grad=True),\n","     \"w_2\": torch.tensor(1.0, requires_grad=True),\n","     \"w_3\": torch.tensor(1.0, requires_grad=True)}\n","optimizer = torch.optim.SGD(w.values(), lr=0.01)\n","print(optimizer)\n","\n","loss = (w[\"w_0\"] + 2 * w[\"w_1\"] + 3 * w[\"w_2\"] + 4 * w[\"w_3\"]) ** 2 # Perform forward pass\n","loss.backward()                                                     # Perform backward passes\n","\n","print(\"Gradients before zeroing:\")\n","for name, param in w.items():\n","    print(name, param.grad)\n","\n","optimizer.step()\n","optimizer.zero_grad() # Zero the gradients\n","\n","print(\"\\nGradients after zeroing:\")\n","for name, param in w.items():\n","    print(name, param.grad)"],"metadata":{"id":"p_91GgO3Nt1E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686537508957,"user_tz":-540,"elapsed":301,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"306851d7-41b9-4ac3-b1f4-1f43743a8553"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SGD (\n","Parameter Group 0\n","    dampening: 0\n","    differentiable: False\n","    foreach: None\n","    lr: 0.01\n","    maximize: False\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")\n","Gradients before zeroing:\n","w_0 tensor(20.)\n","w_1 tensor(40.)\n","w_2 tensor(60.)\n","w_3 tensor(80.)\n","\n","Gradients after zeroing:\n","w_0 None\n","w_1 None\n","w_2 None\n","w_3 None\n"]}]},{"cell_type":"markdown","metadata":{"id":"hxIIM7t27rQ-"},"source":["# **5. Utilizing GPUs**\n","\n","Deep learning algorithms require a **lot of numerical operations**.\n","And by default these operations are often done on a **CPU** (**computer processing unit**).\n","However, there's another common piece of hardware called a **GPU** (**graphics processing unit**), which is **often much faster** at performing the specific types of operations neural networks need (matrix multiplications) than CPUs.\n","There are a few ways to first get access to a GPU and secondly get PyTorch to use the GPU.\n","\n","> **Note:** When I reference \"GPU\" throughout this course, I'm referencing a [Nvidia GPU with CUDA](https://developer.nvidia.com/cuda-gpus) enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing & not just graphics) unless otherwise specified.\n","\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"59ulvSEhvgoi"}},{"cell_type":"markdown","metadata":{"id":"0UiR6QpoYQH_"},"source":["## **5.1. Getting a GPU**\n","\n","You may already know what's going on when I say GPU. But if not, there are a few ways to get access to one.\n","\n","| **Method** | **Difficulty to setup** | **Pros** | **Cons** | **How to setup** |\n","| ----- | ----- | ----- | ----- | ----- |\n","| Google Colab | Easy | Free to use, almost zero setup required, can share work with others as easy as a link | Doesn't save your data outputs, limited compute, subject to timeouts | [Follow the Google Colab Guide](https://colab.research.google.com/notebooks/gpu.ipynb) |\n","| Use your own | Medium | Run everything locally on your own machine | GPUs aren't free, require upfront cost | Follow the [PyTorch installation guidelines](https://pytorch.org/get-started/locally/) |\n","| Cloud computing (AWS, GCP, Azure) | Medium-Hard | Small upfront cost, access to almost infinite compute | Can get expensive if running continually, takes some time to setup right | Follow the [PyTorch installation guidelines](https://pytorch.org/get-started/cloud-partners/) |\n","\n","There are more options for using GPUs but the above three will suffice for now. If you're looking to purchase a GPU of your own but not sure what to get, [Tim Dettmers has an excellent guide](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/).\n"]},{"cell_type":"markdown","source":["The \"`!nvidia-smi`\" command is commonly used to check the status and details of NVIDIA GPUs, including GPU utilization, memory usage, temperature, and driver version. It provides an overview of the GPU resources and can be helpful for troubleshooting, monitoring GPU usage, or verifying the installation of GPU drivers.\n","\n","To check if you've got access to a Nvidia GPU, you can run `!nvidia-smi` where the `!` (also called bang) means \"run this on the command line\"."],"metadata":{"id":"jk7rRXc3gjmN"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEMcO-9zYc-w","outputId":"216289c8-3091-4724-9f2d-97c772e338f3","executionInfo":{"status":"ok","timestamp":1686277324792,"user_tz":-540,"elapsed":5,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Jun  9 02:23:07 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   53C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"HvkB9p5zYf8E"},"source":["If you don't have a Nvidia GPU accessible, the above will output something like:\n","\n","```\n","NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","```\n","\n","In that case, go back up and follow the install steps.\n","\n","If you do have a GPU, the line above will output something like:\n","\n","```\n","Wed Jan 19 22:09:08 2022\n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","\n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","```"]},{"cell_type":"markdown","metadata":{"id":"UvibZ6e0YcDk"},"source":["## **5.2. Getting PyTorch to run on GPU**\n","\n","Once you've got a GPU ready to access, the next step is getting PyTorch to use for storing data (tensors) and computing on data (performing operations on tensors).\n","To do so, you can use the [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) package.\n","Rather than talk about it, let's try it out.\n","You can test if PyTorch has access to a GPU using [`torch.cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OweDLgwjEvZ2","outputId":"92e424a1-5f52-4dfd-e648-d9724cb746b6","executionInfo":{"status":"ok","timestamp":1686277450434,"user_tz":-540,"elapsed":4434,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["# Check for GPU\n","import torch\n","torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"jedZcx2PZFpL"},"source":["If the above outputs `True`, PyTorch can see and use the GPU, if it outputs `False`, it can't see the GPU and in that case, you'll have to go back through the installation steps.\n","\n","Now, let's say you wanted to setup your code so it ran on CPU *or* the GPU if it was available.\n","That way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.\n","Let's create a `device` variable to store what kind of device is available."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"j92HBCKB7rYa","outputId":"b00ab9b7-840f-4f25-c468-95b533de585f","executionInfo":{"status":"ok","timestamp":1686277497377,"user_tz":-540,"elapsed":491,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["# Set device type\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"FjFyPP2WaCch"},"source":["If the above outputs `\"cuda\"` it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output `\"cpu\"`, our PyTorch code will stick with the CPU. Note that, in PyTorch, it's best practice to write [**device agnostic code**](https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code). This means code that'll run on CPU (always available) or GPU (if available).\n","\n","If you want to do faster computing you can use a GPU but if you want to do *much* faster computing, you can use multiple GPUs.\n","You can count the number of GPUs PyTorch has access to using [`torch.cuda.device_count()`](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MArsn0DFTGfG","outputId":"de717df5-bb67-4900-805e-a6f00ad0b409"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of devices\n","torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"xVNf1hiqa-gO"},"source":["Knowing the number of GPUs PyTorch has access to is helpful incase you wanted to run a specific process on one GPU and another process on another (PyTorch also has features to let you run a process across *all* GPUs)."]},{"cell_type":"markdown","source":["By default all tensors are created on the CPU, but you can also move them to the GPU (only if it's available)"],"metadata":{"id":"8rd2F7hwO9Dd"}},{"cell_type":"code","source":["if torch.cuda.is_available:\n","  device = torch.device(\"cuda\")\n","  x = torch.ones(5, device = device)\n","  print(x)\n","  y = torch.ones(5)\n","  print(y)\n","  y = y.to(device)\n","  z = x + y\n","  print(z)\n","  #z = z.numpy() provides error b/c numpy only handle CPU tensors, not GPU's.\n","  # move to CPU again\n","  z = z.to(\"cpu\")  # ``.to`` can also change dtype together!\n","  print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOm3TuGaNtpE","executionInfo":{"status":"ok","timestamp":1653701142663,"user_tz":-540,"elapsed":267,"user":{"displayName":"Awol SEiD","userId":"04573728879877581209"}},"outputId":"91168b53-4ae6-44a5-9622-dd60f0ad4afd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1.], device='cuda:0')\n","tensor([1., 1., 1., 1., 1.])\n","tensor([2., 2., 2., 2., 2.], device='cuda:0')\n","tensor([2., 2., 2., 2., 2.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"XqQLcuj68OA-"},"source":["## **5.3. Putting Tensors (and Models) on GPU**\n","\n","You can put tensors (and models, we'll see this later) on a specific device by calling [`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) on them. Where `device` is the target device you'd like the tensor (or model) to go to.\n","Why do this?\n","GPUs offer far faster numerical computing than CPUs do and if a GPU isn't available, because of our **device agnostic code** (see above), it'll run on the CPU.\n"," **Note:** Putting a tensor on GPU using `to(device)` (e.g. `some_tensor.to(device)`) returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:\n","* `some_tensor = some_tensor.to(device)`\n","\n","Let's try creating a tensor and putting it on the GPU (if it's available)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhI3srFXEHfP","outputId":"8d000249-dda7-445f-ddce-d0d846907622","executionInfo":{"status":"ok","timestamp":1686278784646,"user_tz":-540,"elapsed":4986,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) cpu\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":5}],"source":["# Create tensor (default on CPU)\n","tensor = torch.tensor([1, 2, 3])\n","\n","# Tensor not on GPU\n","print(tensor, tensor.device)\n","\n","# Move tensor to GPU (if available)\n","tensor_on_gpu = tensor.to(device)\n","tensor_on_gpu"]},{"cell_type":"markdown","metadata":{"id":"DxXeRKO0TGfG"},"source":["If you have a GPU available, the above code will output something like:\n","\n","```\n","tensor([1, 2, 3]) cpu\n","tensor([1, 2, 3], device='cuda:0')\n","```\n","\n","Notice the second tensor has `device='cuda:0'`, this means it's stored on the 0th GPU available (GPUs are 0 indexed, if two GPUs were available, they'd be `'cuda:0'` and `'cuda:1'` respectively, up to `'cuda:n'`).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4puyUX4Bci5D"},"source":["## **5.4. Moving tensors back to CPU**\n","\n","What if we wanted to move the tensor back to CPU?\n","\n","For example, you'll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).\n","\n","Let's try using the [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) method on our `tensor_on_gpu`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"id":"3ChSLJgPTGfG","outputId":"16e11323-f721-453a-9dc1-b8b758c6d374","executionInfo":{"status":"error","timestamp":1686278950198,"user_tz":-540,"elapsed":509,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-53175578f49e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}],"source":["# If tensor is on GPU, can't transform it to NumPy (this will error)\n","tensor_on_gpu.numpy()"]},{"cell_type":"markdown","metadata":{"id":"LhymtkRDTGfG"},"source":["Instead, to get a tensor back to CPU and usable with NumPy we can use [`Tensor.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html).\n","This copies the tensor to CPU memory so it's usable with CPUs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gN15s-NdTGfG","outputId":"6b0d7c3e-d53d-4658-d7ca-38d04f16fe0f","executionInfo":{"status":"ok","timestamp":1686279047162,"user_tz":-540,"elapsed":489,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":7}],"source":["# Instead, copy the tensor back to cpu\n","tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n","tensor_back_on_cpu"]},{"cell_type":"markdown","metadata":{"id":"qyzNH5lrTGfH"},"source":["The above returns a copy of the GPU tensor in CPU memory so the original tensor is still on GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5u83PCRTGfH","outputId":"4ee583a8-108e-44e3-ea74-91f9d70bc6ad","executionInfo":{"status":"ok","timestamp":1686279079490,"user_tz":-540,"elapsed":5,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1, 2, 3], device='cuda:0'), array([1, 2, 3]))"]},"metadata":{},"execution_count":9}],"source":["tensor_on_gpu, tensor_back_on_cpu"]},{"cell_type":"markdown","metadata":{"id":"xlmBpnuPTGfH"},"source":["## Exercises\n","\n","All of the exercises are focused on practicing the code above.\n","\n","You should be able to complete them by referencing each section or by following the resource(s) linked.\n","\n","**Resources:**\n","\n","* [Exercise template notebook for 00](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/00_pytorch_fundamentals_exercises.ipynb).\n","* [Example solutions notebook for 00](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/00_pytorch_fundamentals_exercise_solutions.ipynb) (try the exercises *before* looking at this).\n","\n","1. Documentation reading - A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get some things for now, the focus is not yet full understanding, it's awareness). See the documentation on [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) and for [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics).\n","2. Create a random tensor with shape `(7, 7)`.\n","3. Perform a matrix multiplication on the tensor from 2 with another random tensor with shape `(1, 7)` (hint: you may have to transpose the second tensor).\n","4. Set the random seed to `0` and do exercises 2 & 3 over again.\n","5. Speaking of random seeds, we saw how to set it with `torch.manual_seed()` but is there a GPU equivalent? (hint: you'll need to look into the documentation for `torch.cuda` for this one). If there is, set the GPU random seed to `1234`.\n","6. Create two random tensors of shape `(2, 3)` and send them both to the GPU (you'll need access to a GPU for this). Set `torch.manual_seed(1234)` when creating the tensors (this doesn't have to be the GPU random seed).\n","7. Perform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors).\n","8. Find the maximum and minimum values of the output of 7.\n","9. Find the maximum and minimum index values of the output of 7.\n","10. Make a random tensor with shape `(1, 1, 1, 10)` and then create a new tensor with all the `1` dimensions removed to be left with a tensor of shape `(10)`. Set the seed to `7` when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape."]},{"cell_type":"markdown","metadata":{"id":"eDLKScG7-Zb-"},"source":["## Extra-curriculum\n","\n","* Spend 1-hour going through the [PyTorch basics tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) (I'd recommend the [Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) and [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) sections).\n","* To learn more on how a tensor can represent data, see this video: [What's a tensor?](https://youtu.be/f5liqUk0ZTw)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/mrdbourke/pytorch-deep-learning/blob/main/00_pytorch_fundamentals.ipynb","timestamp":1686280063173}],"gpuType":"T4"},"interpreter":{"hash":"3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}